{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKOTlwcmxmej"
   },
   "source": [
    "# BERT Multi-Class Classification Tutorial with Code\n",
    "\n",
    "By Chris McCormick\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8uEW4O7f8XS"
   },
   "source": [
    "This Notebook will show you how to fine-tune BERT for **multi-class text classification** tasks, meaning that there are more than just two classes in the dataset. We'll be using the 20 Newsgroups dataset, which is a test classification dataset with 20 different classes. I'll discuss this dataset in more detail in section 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uramjg4GhXD5"
   },
   "source": [
    "\n",
    "**Multi-Label vs. Multi-Class**\n",
    "\n",
    "It's important to note that \"Multi-Class\" and \"Multi-Label\" actually refer to two *different* kinds of tasks, and are handled differently.\n",
    "\n",
    "When our documents can have a variable number of labels, we call it **multi-label** classification. For example, imagine a system which proposes \"tags\" to add to documents--there can be multiple appropriate tags! \n",
    "\n",
    "When each document only belongs to *one category each*, then we refer to it as either **binary** classification (if there are only two classes), or **multi-class** classification (if there are *more than 2* classes).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BA9fG7zZhZpE"
   },
   "source": [
    "\n",
    "**Fine-Tuning Basics**\n",
    "\n",
    "If you aren't already familiar with the basics of fine-tuning BERT, you might want to check out my Notebook [BERT Fine-Tuning Tutorial with PyTorch](https://colab.research.google.com/drive/1Y4o3jh3ZH70tl6mCd76vz_IxX23biCPP), which serves as a nice introduction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lj_eBt3G3bo6"
   },
   "source": [
    "# Part I - Dataset & Tokenization\n",
    "---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RX_ZDhicpHkV"
   },
   "source": [
    "## S1. Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nSU7yERLP_66"
   },
   "source": [
    "### 1.1. Connecting GPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GI0iOY8zvZzL"
   },
   "source": [
    "\n",
    "Google Colab offers free GPUs and TPUs! Since we'll be training a large neural network it's best to take advantage of this (in this case we'll attach a GPU), otherwise training will take a very long time.\n",
    "\n",
    "This notebook is already set up to use a GPU. If you ever need to add one to a Notebook, though, you do this by going to the menu and selecting:\n",
    "\n",
    "`Edit ðŸ¡’ Notebook Settings ðŸ¡’ Hardware accelerator ðŸ¡’ (GPU)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqG7FzRVFEIv"
   },
   "source": [
    "In order for torch to use the GPU, we need to identify and specify the GPU as the device. Later, in our training loop, we will load data onto the device. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "oYsV4H8fCpZ-",
    "outputId": "6ef0f16a-23bd-4a8b-82f9-63a196807436"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ElsnSNUridI"
   },
   "source": [
    "### 1.2. Installing `transformers` from huggingface\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_N2UDLevYWn"
   },
   "source": [
    "\n",
    "Next, let's install the [transformers](https://github.com/huggingface/transformers) package from Hugging Face which will give us a pytorch interface for working with BERT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "id": "0NmMdkZO8R6q",
    "outputId": "74f16d42-9178-4b6d-85f8-728057123b7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./venv/lib/python3.8/site-packages (4.5.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.8/site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.8/site-packages (from transformers) (20.3)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in ./venv/lib/python3.8/site-packages (from transformers) (0.10.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.8/site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: sacremoses in ./venv/lib/python3.8/site-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./venv/lib/python3.8/site-packages (from transformers) (4.60.0)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.8/site-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: six in ./venv/lib/python3.8/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in ./venv/lib/python3.8/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in ./venv/lib/python3.8/site-packages (from sacremoses->transformers) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bt3HTo7LLMtj"
   },
   "source": [
    "## S2. Retrieve & Inspect Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNgdnNsJO6AJ"
   },
   "source": [
    "The 20 Newsgroups dataset is old (from the 1990s?), and contains what today we would probably call \"forum posts\", coming from 20 different subtopics.  \n",
    "\n",
    "From the dataset [homepage](http://qwone.com/~jason/20Newsgroups/): \n",
    "\n",
    "> \"Here is a list of the 20 newsgroups, partitioned (more or less) according to subject matter:\"\n",
    ">\n",
    "> |                                                                                                                 |                                                                        |                                                             |\n",
    "|-----------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------|-------------------------------------------------------------|\n",
    "| comp.graphics<br>comp.os.ms-windows.misc<br>comp.sys.ibm.pc.hardware<br>comp.sys.mac.hardware<br>comp.windows.x | rec.autos<br>rec.motorcycles<br>rec.sport.baseball<br>rec.sport.hockey | sci.crypt<br>sci.electronics<br>sci.med<br>sci.space        |\n",
    "| misc.forsale                                                                                                    | talk.politics.misc<br>talk.politics.guns<br>talk.politics.mideast      | talk.religion.misc<br>alt.atheism<br>soc.religion.christian |\n",
    "\n",
    "\n",
    "Scikit-learn includes some nice helper functions for retrieving the 20 Newsgroups dataset (from their example [here](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html)). We'll use them below to retrieve the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "50JTGROXMeBZ",
    "outputId": "f18ae6e6-2b41-4c06-c647-cb0a6de15486"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Retrieve the dataset.\n",
    "# The scikit-learn example recommends stripping the metadata from these examples\n",
    "# because it can be too much of a give-away to make it an interesting text \n",
    "# classification problem. That is what the `remove` parameter is doing below.\n",
    "train = fetch_20newsgroups(subset='train',\n",
    "                           remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "test = fetch_20newsgroups(subset='test',\n",
    "                           remove=('headers', 'footers', 'quotes'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_9jkwfgPz99"
   },
   "source": [
    "Let's check out some random examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Njp6-5b5NwiE",
    "outputId": "03c951fe-8961-4b95-8ff1-11c8554f8237"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== alt.atheism ========\n",
      "   So now we're judging the Qur'an by what's not in it?      How many mutton\n",
      "headed arguments am I going to have to wade   through today?     One would hope.\n",
      "/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\   Bob\n",
      "Beauchaine bobbe@vice.ICO.TEK.COM   They said that Queens could stay, they blew\n",
      "the Bronx away, and sank Manhattan out at sea.\n",
      "\n",
      "\n",
      "======== rec.sport.baseball ========\n",
      "was...\n",
      "\n",
      "\n",
      "======== rec.autos ========\n",
      " Oh come on, Silly, all you have to do is cut a hole in your hood and  put a\n",
      "tube there so you can get to the oil fill hole.  What do you think all those big\n",
      "air intake things are for on those hot-rod cars? They're just for looks\n",
      "only...little does anyone know, they provide access to the oil-fill hole.\n",
      "\n",
      "\n",
      "======== comp.os.ms-windows.misc ========\n",
      "Word for Windows lets me designate text as being in a language other than US\n",
      "English.  (Alt-F L, Format | Language, as I recall.)  So I mark it for English\n",
      "(UK), but it still accepts \"rumor\" and squawks at \"rumour\".  As far as I can\n",
      "see, Microsoft didn't include the English (UK) dictionary on my disks.  (I don't\n",
      "mean to imply that I was singled out; I assume that nobody in the U.S got them.)\n",
      "I dialed the Microsoft BBS, but nothing in the Word for Windows section looked\n",
      "helpful.  Can anyone tell me where or how to obtain the UK spelling dictionary\n",
      "for Winword 2.0?  The file name would be SPELL_UK.LEX or something similar.\n",
      "Email please; I'll post a summary.\n",
      "\n",
      "\n",
      "======== comp.os.ms-windows.misc ========\n",
      "My suggestion would be to contact Microsoft about the Video4Windows SDK. You\n",
      "would need to call Developer Services at (800)227-4679 extension 11771 from\n",
      "6:30am to 5:30pm Pacific time.\n",
      "\n",
      "\n",
      "======== rec.sport.baseball ========\n",
      "HEY!!! All you Yankee fans who've been knocking my prediction of Baltimore. You\n",
      "flooded my mailbox with cries of \"Militello's good, Militello's good.\"  Where is\n",
      "he??!! I noticed he got skipped over after that oh so strong first outing.  He's\n",
      "not by any chance in Columbus  now, is he?  Please don't tell me you're relying\n",
      "on this guy to be the *fourth*, not the fifth, but the  *fourth* starter on this\n",
      "brittle pitching staff.    As for the O's, it's still early.   See y'all at the\n",
      "ballyard Go Braves Chop Chop  Michael Mule'\n",
      "\n",
      "\n",
      "======== rec.sport.hockey ========\n",
      " Press conference at 1PM ...   Interestingly, Keenan's co-coach (or is it his\n",
      "\"Number One\"?) on Team Canada at the World Championships is Roger Neilsen.\n",
      "It'd be interesting if the Rangers call in the balance of Neilsen's contract to\n",
      "be Keenan's assistant ...  Roger did do a very good job with the mediocre\n",
      "players, just as he handled the Cinderella Canucks of 10 years ago ... but his\n",
      "mistake was playing the Rangers like those Canucks last May ...  gld\n",
      "\n",
      "\n",
      "======== talk.politics.guns ========\n",
      "\n",
      "\n",
      "\n",
      "======== alt.atheism ========\n",
      " [Dan's article deleted]  I found the same add in our local Sunday newspaper.\n",
      "The add was placed in the ..... cartoon section! The perfect place for it ! :-)\n",
      "\n",
      "\n",
      "======== talk.politics.misc ========\n",
      "THE WHITE HOUSE                      Office of the Press Secretary\n",
      "_____________________________________________________________________ For\n",
      "Immediate Release                                   April 5, 1993\n",
      "PRESS BRIEFING                        BY GEORGE STEPHANOPOLOUS\n",
      "The Briefing Room    10:10 A.M. EDT                            MR.\n",
      "STEPHANOPOULOS:  As you know, the President is soon  to be on his way, on\n",
      "Amtrack to Camden Yards.  He'll be throwing out  the first pitch.\n",
      "Q    It's MARC, the Maryland Area Transit, it's not  Amtrack.\n",
      "MR. STEPHANOPOULOS:  Oh, it's not Amtrack?  Well, he's  going from Union\n",
      "Station, you're right.  Excuse me.                            Q    George, what\n",
      "exactly are you prepared to do to  break the logjam with  ??? Senate?\n",
      "MR. STEPHANOPOULOS:  Well, as you know, there are  discussions between Senator\n",
      "Mitchell and Senator Dole this morning,  and I think that the President is going\n",
      "to continue to make the point  that he believes that our investment package, our\n",
      "jobs package needs  to be passed as quickly as possible.  We need this\n",
      "investment for  summer jobs, for immunization, for highway construction, for the\n",
      "important programs that will put people back to work right away this  summer.\n",
      "And the President continues to believe his program should be  passed.\n",
      "Q    Will he compromise, that's the question?                            MR.\n",
      "STEPHANOPOULOS:  Well, as you know, the discussions  are going on this morning\n",
      "in the Senate betwen Senator Mitchell and  Senator Dole, let's see what they\n",
      "come up with.  But the President  believes his jobs program should go forward.\n",
      "Q    George, would the President be willing to accept $8  billion for one year,\n",
      "which apparently appears to be the compromise  being offered by --\n",
      "MR. STEPHANOPOULOS:  Well, I don't know what is being  offered by either side.\n",
      "The Senate discussions are going on right  now, let's see what happens today.\n",
      "Q    Would he go that far -- no matter what the  Republicans have offered so\n",
      "far, would he go that far, $8 billion per  year?                            MR.\n",
      "STEPHANOPOULOS:  The President believes that his  program should be passed at\n",
      "this time.  Clearly, we're going to be  willing to listen to what the Senators\n",
      "might or might not be able to  come up with, but I'm not going to get into\n",
      "figures right now.  Let's  see what happens.                            Q\n",
      "It's reasonable to assume, isn't it, from what has  happened so far that a\n",
      "compromise is going to be necessary in order  to get a vote?\n",
      "MR. STEPHANOPOULOS:  Well, the Republicans seem more  intrested in stopping\n",
      "progress on the President's jobs bill, than in  doing something to create --\n",
      "MR. STEPHANOPOULOS:  Well, the  Republicans seem more interested in stopping\n",
      "progress on the  President's jobs bill than in doing something to create real\n",
      "action,  real jobs this summer for the American people.  I think there's no\n",
      "question about that.  There's been some frustration of legislative  activity\n",
      "over the last few days.                            Q    So, you'll need to\n",
      "compromise to get your package  through?                            MR.\n",
      "STEPHANOPOULOS:  We'll see what happens with the  conversations between Senator\n",
      "Mitchell and Senator Dole this morning.                            Q\n",
      "prepared to compromise --                            MR. STEPHANOPOULOS:  The\n",
      "President's prepared to listen  to what Senator Mitchell has to say this morning\n",
      "after his meeting  with Senator Dole.                             Q    Does he\n",
      "feel that he has been defeated in his --                            MR.\n",
      "STEPHANOPOULOS:  Not in the least.  In fact, he's  been very successful so far\n",
      "in the beginning of his term.                            Q    How?\n",
      "MR. STEPHANOPOULOS:  He passed his budget in record  time, in six weeks, and\n",
      "it's a budget which reduces the deficit by  $500 billion over five years.  And\n",
      "it's a budget which provides for  important investments in our future.  Right\n",
      "now we've also had strong  passage of his jobs program through the House.\n",
      "Simply because a  minority of Republicans in the Senate choose to perpetuate\n",
      "gridlock  and hold up action on the President's jobs program is not a sign that\n",
      "he is not succeeding overall.                            Q    He can't beat\n",
      "this, can he?                            Q    The fact is they can do that?\n",
      "MR. STEPHANOPOULOS:  Well, the Republicans can stop  action.  There's no\n",
      "question about that.                            Q    What are you going to do\n",
      "about it?                            Q    George, what do you know about these\n",
      "alleged notes  taken by Boris Yeltsin during one of the meetings in which it\n",
      "appears  that the President told Boris Yeltsin not to trust the Japanese; that\n",
      "when they say yes they mean no?                            MR. STEPHANOPOULOS:\n",
      "I think that's a complete  misreading of what happened at the meeting.  The\n",
      "context of the  conversation was that President Clinton was actually reassuring\n",
      "President Yeltsin at the time about his conversations with Prime  Minister\n",
      "Miyazawa over the Kuril Islands and the Prime Minister's  belief that Japan\n",
      "would play a constructive role in the G-7 process.   I mean this was a casual\n",
      "comment about Japanese courtesy and  etiquette but overall it was in the context\n",
      "of a conversation where  he was reassuring President Yeltsin that he believed\n",
      "the Japanese  were serious about their commitment to the G-7 process.\n",
      "Q     Are you saying that the President said that when  the Japanese say yes\n",
      "they mean no?                            MR. STEPHANOPOULOS:  That's not -- I\n",
      "don't know the  exact words and there was a much longer discussion about he did\n",
      "say  something along the lines that he believes that on this issue Prime\n",
      "Minister Miyazawa intends to really go forward with the G-7 process.\n",
      "Q    Have there been any attempts to explain this to the  Japanese because\n",
      "apparently the Japanese press has picked this up and  there appears to be --\n",
      "MR. STEPHANOPOULOS:  I've seen the reports in the  Japanese press and it\n",
      "actually does put the situation in context.  It  does talk about the Japanese --\n",
      "understanding the Japanese points of  view.  I don't think it's going to be a\n",
      "problem.  I believe that  there may have been some diplomatic context just to\n",
      "clear things up  but I'm not positive.                            Q    George,\n",
      "was the specific comment that was made  specific to the Kuril Island situation\n",
      "or was it a general  observation on Japanese etiquette?\n",
      "MR. STEPHANOPOULOS:  The discussion was about --                            Q\n",
      "The specific comment is what I'm asking about that  has alleged to have been\n",
      "translated from the Russian notes, \"when the  Japanese say yes they mean no.\"\n",
      "MR. STEPHANOPOULOS:  Well, it was a combination.  I mean  I don't think that's\n",
      "the whole sentence.  I think that the specific  comment was a broad, general\n",
      "observation followed up by a specific  finish to the sentence where he said in\n",
      "this case he believes that  Prime Minister Miyazawa means to keep the\n",
      "commitment.                            Q    Was he saying it facetiously first?\n",
      "MR. STEPHANOPOULOS:  I think it was just a casual  observation.\n",
      "Q    And then you say diplomatic contacts were made to  clear it up.  Has the\n",
      "President sent a message?                            MR. STEPHANOPOULOS:  No, I\n",
      "don't think the President  spoke; I believe that Secretary Christopher has made\n",
      "some calls but  I'm not sure exactly how many.                            Q\n",
      "Well, this obviously is a bigger deal than you're  making it out to be if\n",
      "Christopher has had to make some calls.                            MR.\n",
      "STEPHANOPOULOS:  No, no.  It was just that we got  the reports in Vancouver and\n",
      "the Secretary wanted to make sure that  it was understood and make sure there\n",
      "was absolutely no               MR. STEPHANOPOULOS:  -- reports in Vancouver,\n",
      "and the  Secretary wanted to make sure that it was understood, and make sure\n",
      "there is absolutely no misunderstanding, and I don't believe there is  on.\n",
      "Q    What is our position about the Japanese?  That they  may have to say one\n",
      "thing, but actualy mean another?                            MR. STEPHANOPOULOS:\n",
      "No.  The position on the Japanese  is as the President stated to President\n",
      "Yeltsin throughout the two  days.  He said that he had had a good conversation\n",
      "with Prime  Minister Miazawa prior to the Summit.  He reiterated the U.S.\n",
      "position, the long-standing support for the Japanese position on the  Kuril\n",
      "Islands, but also reiterated Prime Minister Miazawa's  commitment to move\n",
      "forward on the G-7 process and to play a  constructive role.  And I think\n",
      "President Yeltsin was very glad to  hear that.                            Q\n",
      "After listening to Secretary Christopher on Iraq  for the last few days, I'm a\n",
      "little confused.  What is the U.S.  policy?  Do you want to see Saddam Hussein\n",
      "overturned?                            MR. STEPHANOPOULOS:  It's the same policy\n",
      "that Secretary  Christopher has reiterated, and all of the U.S officials have\n",
      "reiterated.  We expect full and complete and unequivocal compliance  with all\n",
      "U.N. resolutions.  Right now we do not have that compliance.\n",
      "Q       throwing it out further that if Iraq complies,  Saddam can't stay in\n",
      "office?                            MR. STEPHANOPOULOS:  Right.  I think that\n",
      "that's -- our  judgment is that it is not possible for Saddam Hussein to comply\n",
      "with  the resolutions and stay in power.  But the important point is that  we\n",
      "expect compliance by Iraq with all U.N. resolutions, and we will  continue to\n",
      "demand it.                            Q    And are you concerned the Iran will\n",
      "become the  dominant power in the area --                            MR.\n",
      "STEPHANOPOULOS:  Secretary Christopher has also  spoken to Iran over the last\n",
      "several days, and he says we also expect  full Iranian compliance with all\n",
      "international norms, and stopping  support of terrorism.\n",
      "Q    That's a very glib statement that he won't stay in  power if he complies\n",
      "with U.N. resolutions.  On what logic do you  base that?\n",
      "MR. STEPHANOPOULOS:  Right now Saddam Hussein is not  complying with the U.N.\n",
      "resolutions at all.  He is not respecting the  rights of his people, as is\n",
      "required by the U.N. resolution.  He is  not fully complying with all the\n",
      "resolutions regarding inspections.   He is not fully complying with all the\n",
      "resolutions regarding  armaments.                            Q    Well, when do\n",
      "you think that if he did comply he  would be out of power?               MR.\n",
      "STEPHANOPOULOS:  Well, right now his power rests on  the repression of his\n",
      "people.  If he stopped doing that it would make  it more difficult for him to\n",
      "stay in power.                            Q    George, back on the stimulus\n",
      "package, why is it  that you and the President accuse the Republicans of playing\n",
      "pure  politics and perpetuating gridlock?  Why is it that -- what evidence  do\n",
      "you have that they just don't have a genuine idealogical  difference with you\n",
      "that's in good faith?                            MR. STEPHANOPOULOS:  Well, the\n",
      "fact that several times  in the past the Republicans, many of the ones who are\n",
      "now leading the  fight for the filibuster, have supported the very funding they\n",
      "now  seek to stop, most especially, the highway funding.\n",
      "Q    George, in regards to that, some of the moderate  Republicans said that the\n",
      "White House erred by not being more open to  them during the -- while the plan\n",
      "was put together, that they had  one, sort of, proforma meeting between the\n",
      "White House and the Senate  Republicans, and that was it.  Does the\n",
      "Administration look back and  thinks perhaps it could have done a better job of\n",
      "working with some - -                            MR. STEPHANOPOULOS:  Well, I'm\n",
      "not sure that it's true  that there was only one meeting.  I mean, the President\n",
      "met with the  Republican leadership on at least two occasions before the\n",
      "introduction of his package.  He met with the entire Senate  Republican Caucus\n",
      "also for lunch, and went up there.  We are  continually in contact with as many\n",
      "Republicans as we can find who  have an interest in the President's package.  We\n",
      "are interested in  what they have to say, as well.  But we believe that this\n",
      "program is  important, and we're going to continue to fight for it.\n",
      "Q       your all or nothing, do it with the Democrats  alone strategy, did you\n",
      "maybe miscalculate the ability to get it  through?\n",
      "MR. STEPHANOPOULOS:  Well, I mean, I think that there is  no question that under\n",
      "the Senate rules a determined minority can  frustrate activity.  I mean, there\n",
      "is just no question about that.   You only need 40 plus one to keep going.  40\n",
      "plus one to keep going  and to stop any action, and that's what the Republicans\n",
      "are doing.                            Q       going to rethink the way you\n",
      "attempt to get  other things passed as you go through this process for the rest\n",
      "of  the summer?                            Q       work with Republicans and try\n",
      "to woo some  Republicans into your camp?                            MR.\n",
      "STEPHANOPOULOS:  I think we're going to continue to  look for the support of\n",
      "Republicans whenever we can get it on the  President's intiatives?\n",
      "Q    But on this one -- how are you going to do it  differently than you did it\n",
      "on this one because on this one you  really did stiff the Republicans from the\n",
      "beginning and made it clear  that it was a Democratic majority that would get\n",
      "this through and  could get it through and you really didn't need Republican\n",
      "votes?   Are you going to take a different tack when you have to go for\n",
      "particular votes?  When you have to go through --                            MR.\n",
      "STEPHANOPOULOS:  I can't see into the future and  understand every possible turn\n",
      "in the legislative road.  Clearly the  President's going to continue to reach\n",
      "out when he can.                            Q    You don't have any regrets then\n",
      "about the way you  have handled it up to now and you don't plan any changes in\n",
      "your  approach in dealing with the Republicans in Congress based on this\n",
      "experience?                            MR. STEPHANOPOULOS:  Obviously the\n",
      "President would like  his package passed as quickly as possible and he's going\n",
      "to continue  to press for that.  We will continue to reach out to Republicans,\n",
      "there's not question about that.  And we'll continue to reason with  them and\n",
      "try and find appropriate avenues for cooperation.  In this  case the Republicans\n",
      "have chosen to unify around a filibuster, around  a plan to frustrate action not\n",
      "a plan to move forward.                            Q    They're being denied any\n",
      "other legislative means of  putting their proposals forward.\n",
      "MR. STEPHANOPOULOS:  I think they're being --                            Q\n",
      "any ideas.                            MR. STEPHANOPOULOS:  I think their\n",
      "amendments are being  defeated; I don't know that they're being denied.\n",
      "Q       to present them.                            MR. STEPHANOPOULOS:  That's\n",
      "not exactly true.  I mean  they get the votes --                            Q\n",
      "that theirs can be passed though by the  parliamentary rules under which they're\n",
      "playing.                            MR. STEPHANOPOULOS:  Unless they get a\n",
      "majority in  support all the way around, no, that's not exactly true.\n",
      "Q    George, one more on Iraq.  Is the administration  backing any of the Iraqi\n",
      "opposition?  Grooming any new leadership?                            Q\n",
      "backing any of the Iraqi -- leadership?                            MR.\n",
      "STEPHANOPOULOS:  Oh, I -- again, we're pressing for  Iraqi compliance.  I don't\n",
      "know if we can get into the business of  grooming leadership.  I believe there\n",
      "have been some contacts, at  some levels, with Iraqi opposition groups.  I don't\n",
      "know  about  anything recently.                            Q       Jesse\n",
      "Jackson, who, of course, is not the  President's best friend, has, however, been\n",
      "told that there is to be  some kind of town meeting, or some kind of involvement\n",
      "by the  President, pre-empted the ball game -- Los Angeles.  Will he consider\n",
      "something like that, or any other kind of intervention there?\n",
      "MR. STEPHANOPOULOS:  Well, as you know, the President  appointed Commerce\n",
      "Secretary Ron Brown, about 10 days ago, to be a  special envoy to California,\n",
      "and coordiante cabinet activities around  the California economic situation,\n",
      "including the situation in Los  Angeles.                              I believe\n",
      "there will also be visits out to Los Angeles  by the Education -- or have been\n",
      "visits by the Education Secretary,  Mr. Riley.  I believe that Transportation\n",
      "Secretary Pena and HUD  Secretary Cisneros are also going out.  And there may be\n",
      "other visits  by Cabinet officials over the next several days and weeks.  I\n",
      "wouldn't rule out the possibility of a visit by President Clinton to\n",
      "California.  Obviously, he is following the situation closely, and is  concerned\n",
      "about making sure that we make the right long term policy  decisions that will\n",
      "help create the kind of economic opportunities  which help prevent disturbances.\n",
      "But we're going to continue to  watch it.                            Q\n",
      "George, as a follow-up, Reverend Jackson is also  supposed to be outside the\n",
      "ball park today, in Baltimore, with a  group of supporters protesting the lack\n",
      "of minorities in baseball  management.  Does the President have a position on\n",
      "that?                            MR. STEPHANOPOULOS:  The President has received\n",
      "correspondence from Reverend Jackson.  I know that Reverend Jackson  has also\n",
      "spoken with the White House Chief of Staff, Mack McLarty.   He clearly raises\n",
      "serious questions.  There has been some progress in  baseball over the last\n",
      "several years, but still not enough.  But the  President intends to continue to\n",
      "go to the ball game.                            Q    Is he going to say anything\n",
      "about it today, or see  Reverend Jackson while he's out there?\n",
      "MR. STEPHANOPOULOS:  I don't know if he is going to see  them, but as I said,\n",
      "the President believes that Reverend Jackson has  raised some serious questions,\n",
      "and it's something that, as I said,  even though there has been progress, it's\n",
      "clearly not enough.                            Q    Did Reverend Jackson ask him\n",
      "not to go to the ball  game?                            MR. STEPHANOPOULOS:  I'm\n",
      "not sure about that.  I believe  the characterization the Reverend Jackson is\n",
      "talking about is an  informational pickett.  I don't know that he asked him not\n",
      "to go to  the ball game, but he sent a long, detailed, formal letter outlining\n",
      "his concerns with the situation in major league baseball and the  President read\n",
      "it.                            Q    George, the Orioles are playing the Rangers,\n",
      "the  managing partner of the Rangers is George W. Bush.  Is he going to be\n",
      "there, and is he going to meet with the President?\n",
      "MR. STEPHANOPOULOS:  I don't know.                            Q    What is the\n",
      "Mubarak schedule?                            MR. STEPHANOPOULOS:  I know that\n",
      "President Mubarak is  coming tomorrow morning for a working meeting, they will\n",
      "have a  lunch, and I believe that he is having dinner tonight with Vice\n",
      "President Gore.                            Q    And joint statements tomorrow --\n",
      "MR. STEPHANOPOULOS:  I believe so, yes.  At the end,  yes.\n",
      "Q    Is there evidence, George, that the Egyptians did  warn the U.S. about a\n",
      "potential terrorist bombing -- terrorist  activities?\n",
      "MR. STEPHANOPOULOS:  As reported in The New York Times,  I mean, I think that\n",
      "President Mubarak did say that there has been  general conversations with the\n",
      "Egyptians, as there have been for a  long period of time.  We do have general\n",
      "intelligence sharing, I  mean.  But President Mubarak was very careful to point\n",
      "out that there  was no specific information on this visit that was passed\n",
      "forward.   The President will continue to investigate the situation, but he also\n",
      "reiterates his belief that we cannot tolerate terrorism of any kind.\n",
      "Q    George, he did make specific -- or the Egyptians,  apparently, did issue\n",
      "specific warnings about this individual who,  forgive me --this individual who,\n",
      "forgive me the name escapes me at  the moment, and said the Egyptians were more\n",
      "or less rebuffed in  their attempts to get some kind of action.\n",
      "MR. STEPHANOPOULOS:  Again, I don't know if I would  agree with your\n",
      "characterization of the Mubarak interview.  He did  say that they gave general\n",
      "warnings about the possibility of a  network in the United States and upon which\n",
      "we took appropriate  action.  But there was no specific information on this\n",
      "specific  operation at all.                            Q    So, the White House\n",
      "doesn't feel that any of the  law enforcement agencies whether it be the CIA or\n",
      "FBI who would have  received this kind of information was lax or derelict in its\n",
      "duty in  not pursuing some kind of --                            MR.\n",
      "STEPHANOPOULOS:  No, not at all.                            Q    What's next\n",
      "with Serbia?  It got only a passing  mention in the news conference yesterday --\n",
      "MR. STEPHANOPOULOS:  You didn't get to ask your  question.\n",
      "Q    Yes, exactly.  Was there any agreement on concerted  action between the two\n",
      "countries?  And even if there wasn't, what  does the U.S. do next?\n",
      "MR. STEPHANOPOULOS:  I think the U.S. is clearly going  to move forward in the\n",
      "U.N. today continuing discussions with our  allies on a sanctions resolution and\n",
      "we'll continue to look for ways  to press the Serbians to come to the\n",
      "negotiating table and sign an  agreement.                            Q\n",
      "George, why do you think sanctions is still an  option?  I mean the Serbians\n",
      "make it clear that at least the  leadership is surviving just fine and they feel\n",
      "like they can wait  you out and even the administration officials we had in the\n",
      "other day  said there's no evidence that they're going to have an effect any\n",
      "time soon.  The Bosnian Serbs have said no to the peace plan.  When  does no\n",
      "mean no and you have to do something different?                            MR.\n",
      "STEPHANOPOULOS:  Well, I mean we are doing something  different.  We're moving\n",
      "forward on further sanctions through the  U.N. and those discussions will\n",
      "continue.  We're going to continue to  try and tighten the noose on Serbia, and\n",
      "I think that every  opportunity we have to do that will have an effect over\n",
      "time.                            Q    Are we looking again at lifting the arms\n",
      "embargo?  MR. STEPHANOPOULOS:  The President has said that this is something\n",
      "that is under consideration.                            Q    George, do you have\n",
      "any more on Hugh Rodham's  condition, how he's doing?\n",
      "MR. STEPHANOPOULOS:  As far as I know nothing's changed.\n",
      "Q    George, -- week after Mr. Mubarak?                            MR.\n",
      "STEPHANOPOULOS:  It's a little unclear.  I think  we'll be able to get you more\n",
      "either tonight or tomorrow morning  after the Mubarak visit.\n",
      "Q    Is he going somewhere for Easter?                            MR.\n",
      "STEPHANOPOULOS:  Not that I know of.                            Q    What more\n",
      "can you tell us about the additional aid  to Russia that the President plans to\n",
      "ask Congress about?                            MR. STEPHANOPOULOS:  He's going\n",
      "to be consulting with  the Congress and with our G-7 partners over the next\n",
      "couple of weeks.   I know that he spoke last evening with Congressman Gephardt\n",
      "and their  delegation before the -- the congressional delegation meets with the\n",
      "Russians this week and those consultations will continue over the  next several\n",
      "weeks.                            Q    Do you expect that package to be of the\n",
      "magnitude  of the one announced Sunday?                            MR.\n",
      "STEPHANOPOULOS:  I'm not going to discuss the  magnitude.\n",
      "Q    How about the list of Cold War restrictions, where  do you stand on that --\n",
      "MR. STEPHANOPOULOS:  As the President said yesterday,  he's going to be looking\n",
      "for that list from the Congress this week  and reviewing it.  He believes --\n",
      "he's going to try and get it this  week and he's going to review the list, and\n",
      "we're going to take a  hard look at it.                            Q    But\n",
      "they're making it up?  I mean it's no White  House involvement, Congress is\n",
      "compiling this list?                            MR. STEPHANOPOULOS:  I think\n",
      "he's going to talk to the  congressional leaders about compiling the list but\n",
      "I'm certain we'll  be able to get our own researchers working as well.\n",
      "Q    George, isn't lifting the arms embargo more of a  probability than a\n",
      "possibility?                            MR. STEPHANOPOULOS:  It's something\n",
      "that's under  discussion.                            Q    Secretary Christopher\n",
      "has said that it's a matter  of time and -- for months before that happens.\n",
      "MR. STEPHANOPOULOS:  Again, all I can say is that it's  something that the\n",
      "President is reviewing.  Right now we're working  with our allies in the U.N on\n",
      "a sanctions resolution, and we'll  continue to review other matters.\n",
      "Q    George, can you tell us anything about the schedule  this week?  Any\n",
      "travel?                            MR. STEPHANOPOULOS:  They just asked about\n",
      "that.  I  don't have anything more beyond tomorrow's visit with Mubarak right\n",
      "now.                            Q    Are there consultations, George, with any\n",
      "Jewish  American organizations concerning Jackson-Vanick?\n",
      "MR. STEPHANOPOULOS:  As you know the National Conference  of Soviet Jewry has a\n",
      "list of, I believe,               MR. STEPHANOPOULOS:  -- as you know, the\n",
      "National  Conference of Soviet Jewry has a list of, I believe, 200 Refuseniks.\n",
      "We'll certainly take a look at that and continue appropriate  discussions.\n",
      "Thanks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "import random\n",
    "\n",
    "# Wrap text to 80 characters.\n",
    "wrapper = textwrap.TextWrapper(width=80) \n",
    "\n",
    "# Randomly choose some examples.\n",
    "for i in range(10):\n",
    "    \n",
    "    # Choose a random sample by index.\n",
    "    j = random.choice(range(len(train.data)))\n",
    "    \n",
    "    # Get the text as 'x' and the label integer as 'y'.\n",
    "    x = train.data[j]\n",
    "    y = train.target[j]\n",
    "\n",
    "    # Print out the name of the category and the text.\n",
    "    print('')\n",
    "    print('========', train.target_names[y], '========')\n",
    "    print(wrapper.fill(x))\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zU5aPNzzLjeq"
   },
   "source": [
    "Let's look at the classes to see how balanced they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "fcBinIxWLjer",
    "outputId": "ac520996-ad31-427a-90d8-a8e2787c2644"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafael/Documents/project_master_thesis/chemprot_testing_ground/venv/lib/python3.8/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, '# of Training Samples')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (10,5)\n",
    "\n",
    "\n",
    "# Plot the number of tokens of each length.\n",
    "sns.countplot(train.target)\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('# of Training Samples')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kd1aKF3RQzL1"
   },
   "source": [
    "They look pretty even, except for maybe class 19, which corresponds to 'talk.religion.misc'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "WM2D8cKlQ24c",
    "outputId": "974de3a6-d0b8-4152-8f98-619ba50bc58b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'talk.religion.misc'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.target_names[19]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_HzeZqLFWeh"
   },
   "source": [
    "## S3. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_qjYE4eFciL"
   },
   "source": [
    "BERT has a maximum input length of 512 tokens, so we'll need to truncate any comments which are longer than this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMHzhnmuGrpb"
   },
   "source": [
    "### 3.1. Load BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vT0b0-5NGq0y"
   },
   "source": [
    "\n",
    "In order to see the distribution of comment lengths *in terms of BERT tokens*, we'll need to first apply the BertTokenizer to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83,
     "referenced_widgets": [
      "db72db3535264504963af4b5cad88adf",
      "4ddf121894674a5593b5acb284b9bca6",
      "c9b37fb4de0b41459e754ee3b12294ba",
      "9dc1c087b64e4351aa7c897bb1e505b4",
      "a389ea538d714857896b5c56d5a8eb57",
      "891297991ff240b2ba80f3f171558e60",
      "d88d2ed6ca5a4db4975ddbab3c574111",
      "587a51577f7148a4a2c685b6ccd863d0"
     ]
    },
    "id": "Z474sSC6oe7A",
    "outputId": "56f13e65-e340-4078-9202-670c366e2c2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8kEDRvShcU5"
   },
   "source": [
    "### 3.2. Sequence Length Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGkq8hQJfqZY"
   },
   "source": [
    "To decide on a truncation strategy for this dataset, let's first look at the distribution of sequence lengths.\n",
    "\n",
    "To do this, our first step is to tokenize all of the samples in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1M296yz577fV"
   },
   "source": [
    "**Tokenize All Samples**\n",
    "\n",
    "The `tokenizer.encode` function combines multiple steps for us:\n",
    "1. Split the sentence into tokens.\n",
    "2. Add the special `[CLS]` and `[SEP]` tokens.\n",
    "3. Map the tokens to their IDs.\n",
    "\n",
    "In order to explore the distribution of sequence lengths, we will not perform any truncation here. Unfortunately, this results in the tokenizer spitting out a warning for every sample that's longer than 512 tokens. We'll just have to ignore those for now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2bBdb3pt8LuQ",
    "outputId": "d6e30143-8ad4-4e63-92a8-556c3fd4ee63"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (604 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing comments...\n",
      "  Read 0 comments.\n",
      "DONE.\n",
      "    11,314 comments\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "\n",
    "# Record the length of each sequence (in terms of BERT tokens).\n",
    "lengths = []\n",
    "\n",
    "print('Tokenizing comments...')\n",
    "\n",
    "# For every sentence...\n",
    "for sen in train.data:\n",
    "    \n",
    "    # Report progress.\n",
    "    if ((len(input_ids) % 20000) == 0):\n",
    "        print('  Read {:,} comments.'.format(len(input_ids)))\n",
    "    \n",
    "    # `encode` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sen,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        #max_length = 512,          # Truncate all sentences.                        \n",
    "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.\n",
    "    input_ids.append(encoded_sent)\n",
    "\n",
    "    # Record the non-truncated length.\n",
    "    lengths.append(len(encoded_sent))\n",
    "\n",
    "print('DONE.')\n",
    "print('{:>10,} comments'.format(len(input_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_erWZU7hDTB"
   },
   "source": [
    "Let's grab some quick statistics--what are the min, max and median comment lengths?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "qmrQVSsjg6By",
    "outputId": "a3c11656-68c0-405a-b732-0fb6f1a3fd4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Min length: 2 tokens\n",
      "   Max length: 52,886 tokens\n",
      "Median length: 124.0 tokens\n"
     ]
    }
   ],
   "source": [
    "print('   Min length: {:,} tokens'.format(min(lengths)))\n",
    "print('   Max length: {:,} tokens'.format(max(lengths)))\n",
    "print('Median length: {:,} tokens'.format(np.median(lengths)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jIkyE9qjSjk3"
   },
   "source": [
    "What percentage are over the 512 limit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "DsAV58bZSmEt",
    "outputId": "6d4bdbc4-1e7d-48bb-cd4a-c337de5572dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,015 of 11,314 comments will be truncated (8.97%)\n"
     ]
    }
   ],
   "source": [
    "num_over = 0\n",
    "\n",
    "# For all of the length values...\n",
    "for length in lengths:\n",
    "    # Tally if it's over 512.\n",
    "    if length > 512:\n",
    "        num_over += 1\n",
    "\n",
    "print('{:,} of {:,} comments will be truncated ({:.2%})'.format(num_over, len(lengths), float(num_over) / float(len(lengths))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xytsw1oIfnX0"
   },
   "source": [
    "To further analyze it, let's plot the distribution. To keep the scale of the x-axis reasonable, *we'll clip the lengths to 512.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "iLhi9A49zmsy",
    "outputId": "94b0baf2-3d00-4a0e-bde2-ca2dc418bfd8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafael/Documents/project_master_thesis/chemprot_testing_ground/venv/lib/python3.8/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, '# of Comments')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoAAAAFgCAYAAAArYcg8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAABLZElEQVR4nO3deVxU1f8/8NcwbAKjAhLmjvwaUJDFwFwQZBPINbdMRUXNJRfUzOVjH8s+rmhgoJLkUq4ZlKRpuRRaWO6VmoimCLghQhADCsNwf3/4ZWoEdIAZZnRez8eDR8255xze9x6Hec+5954rEgRBABEREREZDCNdB0BEREREDYsJIBEREZGBYQJIREREZGCYABIREREZGCaARERERAaGCSARERGRgWECSEQNRiaTYfPmzRg0aBBefvlleHh4YMiQIdi9ezcqKip0HZ7OZWdnP7VOeHg4nJycGiCaupPJZMjPz1e+nj9/vt7HTGRomAASUYO4fv06Bg8ejOjoaDg5OWH27NmIjIyEmZkZFi1ahLlz58KQlyVdv349xo0bp+sw6u3ixYsICwvD1atXdR0KET2Bsa4DIKLnX2lpKd566y0UFBQgKSkJzs7Oym0RERFYvHgxdu7cCTc3N4wePVqHkerOL7/8AoVCoesw6u3KlSu4d++ersMgoqfgDCARad3OnTuRkZGBBQsWqCR/lebNm4cmTZrg888/10F0RESGhwkgEWnd/v37YWFhgT59+lS73dzcHF988QWSk5NVys+cOYOxY8fC09MTnp6eGD16NE6fPq1SJyAgAB988AESExMREhICNzc3DB48GOfPn0dubi4iIyPh6emJnj17Ijo6WuVaQycnJ3zyySdISEhAr1694O7ujvDwcGRmZiIjIwPjx4+Hh4cHAgICsHXr1ipxf/XVVxg4cCA6deqErl27Yv78+SqzXzdv3oSTkxOSk5MRExMDX19fdOrUCUOHDsWJEydU9uHUqVO4desWnJycEBcXV5fDXMWvv/6KiIgI5fEbN24czp8/X+X4LVq0CF9//TX69OmDTp06oXfv3tixY0eV/o4dO4ahQ4fCw8MDgYGB2LFjBxYuXIiAgAAAQFxcHBYsWAAAGD16tLK80oULFxAeHg43Nzf06NEDy5YtQ2lpqXK7IAhYu3YtQkJC0KlTJ3Tv3h3vvPMO7ty5o5HjQUT/EPFZwESkTYIgwNXVFZ07d8a2bdvUbvf9999j2rRpaNOmDQYPHgwASExMxO3btxEbG4vAwEAAjxKY8vJyVFRUYMyYMRAEAfHx8WjcuDEkEgleeukldO3aFYcOHUJqaipWrFiB1157DcCjBNDe3h6NGjXCyJEjkZeXh40bN8LR0REFBQXo1asXOnbsiMTERFy8eBHbtm1Dly5dAABr165FXFwcQkJC0LVrV+Tk5GD79u1o0qQJkpKSYGNjg5s3byIwMBAtWrRAo0aNMGzYMMjlcmzevBklJSU4evQorK2tceTIEXz44Yf466+/sGDBAjg5OVU7Uwo8ugnk1KlTSE9Pf+LxO378OCZNmgRnZ2f07dsXZWVl+Oqrr3Dr1i1s2bIFXl5eyuMnCAKKi4sxatQoNGvWDLt378bly5eRkJAAPz8/AEBKSgqmTp0KqVSK1157DTk5Odi2bRssLCxgaWmJH374AZcvX8bOnTuxe/duTJ48GZ06dUJQUBDmz5+PPXv2wNLSEv3794ezszOOHj2KlJQUjB49GgsXLgQAxMfHIzY2FiNHjoSTkxNu3ryJrVu3onnz5vjmm28gFovV/vdDRE8hEBFpUV5eniCVSoVZs2ap3UYulwu+vr6Cn5+fUFRUpCwvLCwUevbsKfTs2VMoKysTBEEQ/P39BScnJ+Hy5cvKeitXrhSkUqkwc+ZMZVlxcbHg4uIizJ49W1kmlUoFd3d3ITc3V1k2Y8YMQSqVCqtWrVKW3bhxQ5BKpUJ0dLQgCIKQlZUlODs7C6tXr1aJOz09XXBxcRGWLl0qCIIgZGdnC1KpVPDz8xOKi4uV9fbv3y9IpVJh9+7dyrJRo0YJ/v7+Tz02o0aNEqRS6RPrKBQKITAwUBg+fLhQXl6ucgyCg4OFAQMGKMsqj19aWpqy7N69e4KTk5PKsQoKChJ69+4tPHjwQFl2+PBhQSqVqsT95ZdfClKpVDhx4oSybN68eYJUKhW2bNmiEmNwcLDg5+enLAsLCxMmTpyosi+7du0S+vfvL2RmZj5xn4modngKmIi0ysjo0Z+Z2tzgcOnSJdy9excjR46ElZWVsrxx48YYNWoUcnJycPHiRWV5mzZtVJYZcXBwAAAEBwcryywsLGBra4vc3FyV3+Xp6YlmzZopX7dr165K21atWgGA8vTu4cOHUVFRgYCAAOTn5yt/mjVrhg4dOuDo0aMqv8PPzw8WFhbK15Wze4/HoimXLl1CdnY2goKCUFhYqIzv4cOH8Pf3R1paGnJycpT1HRwcVGYc7ezs0KxZM9y/fx8AcPnyZWRlZWH48OEwNzdX1gsKCkL79u3VjuvflwAYGRmhY8eOyt8BAM2bN8fJkyfx2WefKcuHDx+Or7/+Gm3atKn9gSCiGvEuYCLSqiZNmsDExERlXbinuXnzJoB/Erl/q0w4bt++DU9PTwCAra2tSp3KU4U2NjZVyoXHrnp5vK2xsXGVtpX9VbbNysoC8Cg5qY6JiYnK68fjMDU1BQCtrX1YGV9UVBSioqKqrXP79m3Y29tXG19ljJXxZWZmAgDatm1bpV779u2RlpamVlyPH2tzc3PI5XLl67lz52LKlClYtmwZli9fDhcXFwQEBGDYsGGws7NT63cQkXqYABKRVolEInh6euLixYsoLy9XJliPi4mJQXZ2NhYsWPDE9QArt/07yaqpT5FI9NT46tK2MjGKj49XmRGrSeUsaEOpjC8yMhIeHh7V1vn3zN3T4isvLwfwT+L6b2ZmZmrH9bTf4+zsjIMHD+Knn35CSkoKfvrpJ8TGxmLLli3YvXs3HB0d1f5dRPRkTACJSOuCg4Nx6tQp7N+/HwMGDKiy/eHDh0hKSoJCoUDTpk3RsmVLAI8Wj35cRkYGgEenC3WlMr4XX3wRHTp0UNl27NgxldPWulAZn4WFBbp3766y7fz58ygsLFQrca3UunVrAMCNGzfg4+Ojsu3GjRv1C/b/KBQKXL58GVZWVggMDFTe5HPgwAHMmjULiYmJmD9/vkZ+FxFxGRgiagCvv/46WrZsiaioKFy5ckVlm0KhwPvvv4/79+/jzTffhImJCVxcXGBnZ4ddu3ZBJpMp68pkMuzcuRN2dnZwdXVt6N1Q8vf3BwBs2LBBZbYyLS0NU6ZMwWeffVbrPo2MjDR2StjV1RV2dnbYtm0biouLleUymQwzZ87EggULanVHraurK1588UUkJSWhrKxMWf7bb7/h0qVLKnUrZ/lquy8KhQKjR4/GsmXLVMrd3d1V+iUizeAMIBFpnZmZGdauXYtx48ZhyJAh6NevHzp16oSCggJ89913SEtLQ2hoKCIiIgA8Or377rvvYtasWRg8eDCGDBkCAEhKSsK9e/cQGxur04RAKpUiPDwc27ZtQ0FBAYKCglBQUIDt27fD0tISkZGRte7TxsYGp0+fxubNm/Hyyy8rE5+aLFq0qNryESNGwNnZWXn8Bg0ahCFDhsDMzEy5jM7q1atrPPVdHSMjI8yfPx8zZ87E8OHDMWDAAOTn52Pr1q1VTgtXXk+4a9cu3L9/H/369VPrd5iamiI8PBzx8fGYOnUqevbsiYcPH2L37t1o1KiRcikgItIMJoBE1CA6duyIr7/+Gp9++il+/PFHHDhwAIIgwMnJCcuWLcOgQYNUrrsLDQ1FkyZNsH79eqxbtw7GxsZwd3fH0qVLlWvY6dLChQvRvn17fP7551i5ciUkEgm8vLwQGRlZp2vVJkyYgPT0dERHR2PQoEFPTQB3795dbbmvry+cnZ2Vxy8+Ph7r16+HkZERXnrpJcTHxytnMGsjNDQUMTExiI+Px6pVq2Bvb48FCxYgOTlZ5Qafbt26ISwsDCkpKThx4gR69+6t9u+YMWMGmjZtii+//BIrV66EWCxG586dsWrVKl7/R6RhXAiaiIieSKFQoLCwsNq7hfv164fGjRtX++QQItJfvKiCiIieSKFQwNfXt8pp5/T0dFy9ehVubm46ioyI6oqngImI6IlMTU0RGhqKpKQkiEQiuLq64t69e9i1axesra2V124S0bODp4CJiOipHj58iE2bNmHv3r24c+cOJBIJunXrhpkzZyqflEJEzw4mgEREREQGhtcAEhERERkYJoBEREREBoY3gdTSX38Vo6JCO2fNbW2tkJcne3pF0jqOhX7heOgPjoX+4FjoF30bDyMjEaytLWvczgSwlioqBK0lgJX9k37gWOgXjof+4FjoD46FfnmWxoOngImIiIgMDBNAIiIiIgPDBJCIiIjIwDABJCIiIjIwTACJiIiIDAwTQCIiIiIDwwSQiIiIyMAwASQiIiIyMEwAiYiIiAwMnwRCREREVAvlFUCpvFylTMgvQUlpeQ0tqjIzMYaxDqfhmAASERER1UKpvByn03JUyiRW5iiSPVS7D+8O9jA2010axlPARERERAaGCSARERGRgWECSERERGRgmAASERERGRgmgEREREQGhgkgERERkYFhAkhERERkYJgAEhERERkYJoBEREREBkZvEsC0tDS4uLjg7t27KuWpqakYPHgw3N3dERAQgM2bN1dpe+HCBYSHh8PT0xM+Pj6Ijo6GXC5XqXPjxg1MnjwZXl5eeOWVV/Dee+9BJpNpdZ+IiIiI9JFePAru2rVrmDRpEsrLVZ+hd+7cOUyePBlhYWGIjIzE2bNnERUVBUEQMH78eABAZmYmxo4dC09PT6xZswbXrl1DTEwMZDIZFi1aBAAoLCzEmDFjYGdnh5UrVyIvLw+rVq3C3bt3sWHDhgbfXyIiIiJd0mkCWF5ejt27d+PDDz+EiYlJle2xsbHo2LEjVq1aBQDw9fVFeXk5Pv74Y4SHh8PU1BQJCQmQSCRYv349TE1N4efnB3NzcyxZsgSTJk2Cvb09duzYgb///hvJycmwtrYGANjb22PixIn4/fff4e7u3qD7TURERKRLOj0FfPbsWaxevRrjxo3DnDlzVLaVlpbizJkz6N27t0p5SEgI/v77b5w7dw4AcPz4cfj7+8PU1FRZJzQ0FAqFAqmpqco63t7eyuQPAHx8fGBpaYljx45pa/eIiIiI9JJOE0BHR0ccOXIE06ZNg1gsVtmWnZ0NuVwOBwcHlfK2bdsCADIyMvDgwQPcuXOnSh0bGxtYWVkhIyMDAHD9+vUqdcRiMVq1aqWsQ0RERGQodHoKuFmzZjVuKyoqAgBYWVmplFtaWgIAZDJZjXUq61Xe5FFUVPTUOkRERESGQi9uAqmOIAhP3G5kZKRWnadRp86/2dpWTSQ1yc5OotX+SX0cC/3C8dAfHAv9wbHQDSG/BBIr8yrl1ZXVxMLCDHY2FpoMq1b0NgGUSB79oy4uLlYpr5yxk0gkylm9x+tU1qvsw8rKqsY6LVq0qFVceXkyVFQ8OfGsKzs7CXJzi7TSN9UOx0K/cDz0B8dCf3AsdKektBxFsocqZRIr8yplT+yjpBS5CoWmQ1MyMhI9cdJKb9YBfFybNm0gFouRlZWlUl752sHBAZaWlrC3t0dmZqZKnby8PBQXFyuv+3NwcKhSR6FQ4ObNm1WuDSQiIiJ63ultAmhmZgYvLy8cOnRI5VTvwYMHIZFI4OrqCgDo0aMHUlJSUFZWplJHLBajS5cuyjonT55EQUGBsk5qaipKSkrQvXv3htkhIiIiIj2htwkgAEyZMgXnzp3DrFmzcOzYMaxZswabNm3CpEmT0KhRIwDAhAkTkJubi4kTJyIlJQVbtmzB8uXLMWzYMOXp3REjRsDU1BRjx47F4cOHkZiYiHfeeQe+vr7o3LmzLneRiIiIqMHpdQLYrVs3xMXF4dq1a5g6dSr27duHuXPn4s0331TWcXR0xObNm1FSUoIZM2Zgy5YtiIiIwMKFC5V1bGxssHXrVjRt2hRz5sxBTEwMQkNDERMTo4vdIiIiItIpkfC0W2lJBW8CMQwcC/3C8dAfHAv9wbHQneLScpxOy1Epq+1NIN4d7GFppr17cZ/Zm0CIiIiISDuYABIREREZGCaARERERAaGCSARERGRgWECSERERGRgmAASERERGRgmgEREREQGhgkgERERkYFhAkhERERkYLS3BDU9s8orgFJ5eb36MDMxhjG/XhAREeklJoBURam86iNuasu7gz2MtfiIGyIiIqo7ztEQERERGRgmgEREREQGhgkgERERkYFhAkhERERkYJgAEhERERkYJoBEREREBoYJIBEREZGB4UJtpBUiIxGKS+u+mDQXkiYiItIeJoCkFaVyBX6/klvn9lxImoiISHs4x0JERERkYJgAEhERERkYJoBEREREBoYJIBEREZGBYQJIREREZGB4myXpJS4jQ0REpD1MAEkvcRkZIiIi7eEcCREREZGBYQJIREREZGCYABIREREZGCaARERERAaGCSARERGRgWECSERERGRgmAASERERGRgmgEREREQGhgkgERERkYF5JhLAXbt2ISwsDB4eHujXrx/27t2rsj01NRWDBw+Gu7s7AgICsHnz5ip9XLhwAeHh4fD09ISPjw+io6Mhl8sbaheIiIiI9IbeJ4C7d+/G+++/j169emH9+vXo3r073nnnHXz77bcAgHPnzmHy5Mlo37494uLi0K9fP0RFRWHTpk3KPjIzMzF27FiYmZlhzZo1GDduHLZs2YLly5frareIiIiIdEbvH5a6Z88evPLKK5g3bx4AoHv37rh48SJ27tyJsLAwxMbGomPHjli1ahUAwNfXF+Xl5fj4448RHh4OU1NTJCQkQCKRYP369TA1NYWfnx/Mzc2xZMkSTJo0Cfb29rrcRSIiIqIGpfczgKWlpbC0tFQpa9q0KQoKClBaWoozZ86gd+/eKttDQkLw999/49y5cwCA48ePw9/fH6ampso6oaGhUCgUSE1N1f5OEBEREemReieAV69exbVr1zQRS7VGjx6Nn376Cd9++y1kMhm+++47HD16FAMGDEB2djbkcjkcHBxU2rRt2xYAkJGRgQcPHuDOnTtV6tjY2MDKygoZGRlai52IiIhIH6l9ClgQBHzyySfIyMjA8uXLUVFRgcmTJ+Onn34C8OjUbGxsbJXZuvrq06cPTpw4gZkzZyrLXnvtNUyYMAG//vorAMDKykqlTWUMMpkMRUVF1daprCeTyWoVj61t1X40yc5OotX+1SHkl0BiZV6vPkxMjOvVR33bW1iYwc7Gos7tAf0YC/oHx0N/cCz0B8dCN2r6nKzN55YmPqfqQ+0EcNOmTYiOjkbPnj0BAN9++y1+/PFHhISE4KWXXsInn3yCdevWYe7cuRoNcMqUKfj111+xYMECdOzYEb///jvWr18PKysrvPrqq09sa2RkBEEQnlqnNvLyZKioeHKfdWVnJ0FubpFW+q6NktJyFMke1qsPubx+fdS3fUlJKXIVijq315exoEc4HvqDY6E/OBa6U93npMTKvFafW/X9nHoaIyPREyet1E4A9+zZg+DgYMTFxQEADhw4gEaNGmHlypUwNzdHcXExvvvuO40mgOfOnUNqaiqWL1+OQYMGAQC6dOmCxo0bY9GiRRgyZAgAoLi4WKVd5ayeRCJRzvw9XqeynkTCb09ERERkWNSe/srOzoavry8AQC6X45dffkGXLl1gbv5outPR0RH379/XaHC3b98GAHTu3Fml3MvLCwCQlpYGsViMrKwsle2Vrx0cHGBpaQl7e3tkZmaq1MnLy0NxcXGVawOJiIiInndqJ4CNGzdWzqydPHkSJSUlyoQQeJR0NWvWTKPBVSZnZ8+eVSn/7bffAADt27eHl5cXDh06pHKq9+DBg5BIJHB1dQUA9OjRAykpKSgrK1OpIxaL0aVLF43GTERERKTv1D4F7Onpie3bt6Nly5b4+OOPYWxsjN69e0MulyMlJQW7du1CUFCQRoNzcXFBUFAQli5diqKiInTo0AEXL17EunXr4OvrC3d3d0yZMgURERGYNWsWXnvtNfz666/YtGkT3n77bTRq1AgAMGHCBOzfvx8TJ07EmDFjcOPGDURHR2PYsGFo0aKFRmMmIiIi0nci4Wl3SfyfO3fuYPz48bh+/TpEIhHmzp2LiIgInDx5EmPGjIGDgwO2bNmC5s2bazTAsrIyrF27Fnv37kVeXh5atmyJvn37YuLEicp1/Q4fPozY2FhkZGTA3t4eI0eOxLhx41T6OXPmDKKiopCWlgZra2sMHDgQ06dPh4mJSa3iMYSbQIpLy3E6LadefbhL7fD7lVydtffuYA9Ls7qvc64vY0GPcDz0B8dCf3AsdKe6z8na3gRS38+pp3naTSBqJ4AAUF5ejkuXLsHe3l759IzCwkLlQsuVM27PMyaA6mECSJrE8dAfHAv9wbHQnechAVT7GsC1a9fi+vXrcHNzU3l0WpMmTfDqq6/i6tWreO+99+oXLRERERFpXa0SwCtXrtS4/ezZs/jqq680EhQRERERaU+Nc4/Z2dkYP348FP9apHDZsmWIiYmpUlcQBNy7dw/t2rXTSpBEREREpDk1JoCtW7fGwIED8csvvwAAbt26haZNm8LW1rZKXbFYDA8PD0yYMEF7kRIRERGRRjzx6sO33noLb731FgAgICAAb7/9NgIDAxskMCIiIiLSDrVvP/nhhx+0GQcRERERNZBa3X9cWFiIQ4cO4f79+yrXBlYSiUSYOnWqxoKjuimvAErl5XVur6VVboiIiEhPqJ0Anjx5EpMnT8bDhw9R09KBTAD1Q6m8fuv4uUvtNBgNERER6Ru1E8APP/wQjRo1wtKlS9GhQwflUziIiIiI6NmidgJ4+fJlREZG4tVXX9VmPERERESkZWovBG1tbQ1jY+09soSIiIiIGobaCeDAgQORmJiI0tJSbcZDRERERFqm9pRe+/btsW/fPoSFhcHPzw82NjYQiUQqdXgTCBEREZH+UzsBnDdvnvL/d+3aVW0dJoBERERE+k/tBPD777/XZhxERERE1EDUTgBbtmypzTiIiIiIqIGofRMIABQUFGDFihUICQmBu7s7fvnlF5w7dw4zZ87EjRs3tBQiEREREWmS2jOAubm5GD58OHJyctCxY0dkZWUBAIqKinD48GH88ssv2LlzJxwdHbUWLJG6REYiFJfW/XF4Qn4JFBWAca2+IhERET0b1E4Ao6OjUVhYiOTkZNjY2KB79+4AAD8/PyQlJWHcuHH46KOPEBsbq7VgidRVKlfg9yu5dW4vsTKHc+smMDbj2pdERPT8UXt+4+jRoxg1ahT+3//7f1WWf+nQoQNGjhyJc+fOaTxAIiIiItIstRPA4uJiNG/evMbt1tbWKCoq0khQRERERKQ9aieAjo6OOHnyZI3bjxw5AgcHB40ERURERETao3YCGB4ejm+//RYxMTHKG0DKyspw+fJlzJ49GydOnMDw4cO1FigRERERaYbaV7gPGjQIt2/fxvr165GQkAAAmDx5MgBAEASEh4czASQiIiJ6BtTqFsdp06ZhwIABOHz4MLKzs6FQKNCqVSv4+/vjpZde0laMRERERKRBtV7jonXr1hg3bpw2YiEiIiKiBlCrBPDMmTNITU1Fbm4uKioqqmwXiURYtmyZxoIjIiIiIs1TOwHctm0bli1bBkEQaqzDBJCIiIhI/6mdAH766adwdXXFhx9+iFatWsHIiM/IIiIiInoWqZ0A5ufnY9KkSWjTpo024yEA5RVAqbzuz7GtqHmSloiIiEj9BLBz5864dOmSNmOh/1MqL8fptJw6t3eX2mkwGiIiInreqJ0Avvvuuxg7diyaNGmCwMBA2NraVnkmMAC0aNFCowESERERkWapnQCKxWI0bdoUCQkJyoWgq5OWlqaRwIiIiIhIO2o1A3jt2jWEhISgXbt2MDau9RKCRM8UkZEIxaV1vxbTzMQYxrxXioiI9JDaWdz58+cxYcIEzJw5U4vhEOmPUrkCv1/JrXN77w72MDbjFyUiItI/as9PWFtbo1mzZtqMpUanT5/GG2+8AXd3d/j4+OB///sfiouLldtTU1MxePBguLu7IyAgAJs3b67Sx4ULFxAeHg5PT0/4+PggOjoacrm8IXeDiIiISC+onQC+8cYb2LFjB/Lz87UZTxW//fYbIiIiYGdnh/j4eEydOhV79+7Fu+++CwA4d+4cJk+ejPbt2yMuLg79+vVDVFQUNm3apOwjMzMTY8eOhZmZGdasWYNx48Zhy5YtWL58eYPuCxEREZE+UPv8lJGREUpKShAYGIjOnTvD1tYWYrFYpY42ngSyevVqeHh44KOPPoJIJEL37t1RUVGBLVu24MGDB4iNjUXHjh2xatUqAICvry/Ky8vx8ccfIzw8HKampkhISIBEIsH69ethamoKPz8/mJubY8mSJZg0aRLs7e01GjMRERGRPlN7BnD16tXIycnBgwcPcPz4cezduxd79uyp8qNJ+fn5OHPmDN544w2VJWdGjhyJI0eOwMjICGfOnEHv3r1V2oWEhODvv//GuXPnAADHjx+Hv78/TE1NlXVCQ0OhUCiQmpqq0ZiJiIiI9J3aM4CXL1/WZhzVunLlCgRBQJMmTTBz5kwcPXoUYrEYffv2xYIFC3Dz5k3I5XI4ODiotGvbti0AICMjA+7u7rhz506VOjY2NrCyskJGRkaD7Q8RERGRPtDrWxQrrzecP38+goODER8fj/T0dKxZswalpaV4/fXXAQBWVlYq7SwtLQEAMpkMRUVF1daprCeTybS5C0RERER6p1YJYHJyMo4fP47c3FxUVFRU2S4SifDZZ59pLLjKu3Q7d+6M9957DwDQrVs3CIKAlStXYtiwYU9sb2RkBEF48oNxjYxqt1CbrW3VRFKT7OwkEPJLILEyr3MfJibGOm2vDzHowz5YWJjBzsaiXjHQP+zsJLoOgf4Px0J/cCx0o6bP6dp8Zuj6M0LtBDAmJgYbNmyAiYkJbG1ta5041UXlTJ6vr69KuY+PD1asWIELFy4AgMqSMACUs3oSiUQ58/d4ncp6Eknt3jx5eTJUVDw5qawrOzsJcnOLUFJajiLZwzr3I5frtr0+xFDf9hIr83r3UVJSilyFos7t6R+V7w3SPY6F/uBY6E51n9MSK/NafWZo+zPCyEj0xEkrtRPAPXv2wMfHB3FxcWjUqJFGgnuadu3aAQDKyspUyitnBlu1agWxWIysrCyV7ZWvHRwcYGlpCXt7e2RmZqrUycvLQ3FxcZVrA4mIiIied2pP48lkMoSEhDRY8gcAjo6OaNmyJQ4cOKBSnpKSAmNjY3h6esLLywuHDh1SOdV78OBBSCQSuLq6AgB69OiBlJQUlUTy4MGDEIvF6NKlS8PsDBEREZGeUDsB7NmzJ06cOKHNWKoQiUSYM2cOzpw5gzlz5uDnn39GQkIC4uPjMWrUKNjY2GDKlCk4d+4cZs2ahWPHjmHNmjXYtGkTJk2apExWJ0yYgNzcXEycOBEpKSnKRaCHDRuGFi1aNOg+EREREema2qeA//vf/yIiIgJvv/02goKCYGtrq7I2XyVvb2+NBvjqq6/C1NQU69atw6RJk2Bra4upU6di0qRJAB7dFBIXF4fY2FhMnToV9vb2mDt3LsaNG6fsw9HREZs3b0ZUVBRmzJgBa2trREREYPr06RqNlYiIiOhZoHYCePv2bRQVFWH//v1VTskCgCAIEIlESEtL02iAABAUFISgoKAatwcHByM4OPiJfXh5eeGLL77QdGhEREREzxy1E8APPvgAf//9N8aPH4927drB2FivlxAkIiIiohqoncVdvXoV06ZNw5tvvqnNeIiIiIhIy9S+CaR58+YNsvYfEREREWmX2hndhAkT8Nlnn+HPP//UZjxEREREpGVqnwK+fPkyRCIR+vfvj9atW6NZs2YQi8UqdTT9KDgiIiIi0jy1E8CUlBSIxWI0b94ccrkcd+7c0WZcRERERKQlaieAP/zwgzbjICIiIqIGUuu1XBQKBS5evIhbt27B1NQUL774IlxcXLQRG9EzTWQkQnFpeZ3bm5kYw5j3XRERkRbUKgFMSUnB4sWLkZOTo3z2rkgkwgsvvID33nsPAQEBWgmS6FlUKlfg9yu5dW7v3cEexmZcb5OIiDRP7U+XM2fOYPr06bC1tcWsWbPg6OgIQRBw/fp17Ny5EzNmzMDWrVvRuXNnbcZLRERERPWkdgIYFxeHli1bIikpCRKJRGXbiBEjMHjwYMTHx+OTTz7ReJBEREREpDlqJ4Dnz5/H1KlTqyR/AGBlZYUhQ4Yw+SPSoPpeQwjwOkIiIqqexi4wEolEkMvlmuqOyODV9xpCgNcREhFR9dSeG3B3d0dSUhJKSkqqbJPJZEhMTESnTp00GhwRERERaZ7aUwPTpk3D6NGj0bdvX4waNQrt2rUDAOVNIDk5OVi8eLG24iQiIiIiDVE7AfTy8kJcXBw++OADREVFQSQSAQAEQYCdnR2io6PRtWtXrQVKRERERJpRq4uDAgMD0atXL/zxxx+4efMmAKBly5ZwcXGBsTGvMyIiIiJ6FtQ6axOLxXBzc4Obmxvy8vLQtGlTiMVibcRGRERERFrw1JtAtm/fjn79+qG8vOpyFMuWLUPPnj3x6aefaiM2IiIiItKCGhNAQRAwd+5cLFmyBPfu3cPt27er1GnVqhWMjIywcuVKzJ49W6uBEhEREZFm1JgAJiYmYu/evRgxYgR+/PFHtGnTpkqdWbNm4fvvv8eAAQPw7bffIjk5WZuxEhEREZEGPDEB9Pb2xqJFi2BmZlZjB2ZmZli2bBmcnZ3x+eefayVIIiIiItKcGhPAP//8E4GBgep1YmSEkJAQpKenaywwIiIiItKOGhNAsVgMU1NTtTuytraGkREfOkpERESk72rM2Nq2bYuLFy+q3dGFCxfQokULjQRFRERERNpTYwLYp08f7Nu3D1evXn1qJ1evXsW+ffvg6+ur0eCIiIiISPNqTABff/11tGjRAuHh4di7dy8UCkWVOhUVFdi3bx8iIiJgaWmJMWPGaDVYIiIiIqq/Gp8EYmlpifj4eLz11luYN28eFi9eDBcXF9jZ2aGiogJ5eXn4448/UFJSghdffBHr1q3DCy+80JCxExEREVEdPPFRcO3bt8fevXuxY8cO7N+/H+fOnVM+EcTExAQeHh7o3bs3Xn/99VrdMEJEREREuvPUZwGbmpoiIiICERERAID8/HyIxWI0adJE68ERUf2IjEQoLq36GEd1mZkYw5g39xMRPXeemgA+zsbGRhtxEJEWlMoV+P1Kbp3be3ewh7FZrf9MEBGRnuN3eyIiIiIDwwSQiIiIyMAwASQiIiIyMDUmgLt27cKNGzcaMBQiIiIiagg1JoBRUVE4c+aM8nVgYCC+//77BgmKiIiIiLSnxtv7TE1NceTIEXh4eKBRo0a4desWbt++jdu3bz+xQ20/D3jatGlIT0/H4cOHlWWpqamIiYnBn3/+CVtbW4waNQrjxo1TaXfhwgVERUXh4sWLsLS0xKBBgzB9+nSYmJhoNV4iIiIifVNjAjhkyBBs2rQJx44dAwCIRCIsW7YMy5Yte2KHaWlpmo3wX77++mscPnwYbdq0UZadO3cOkydPRlhYGCIjI3H27FlERUVBEASMHz8eAJCZmYmxY8fC09MTa9aswbVr1xATEwOZTIZFixZpLV4iIiIifVRjAvjOO+/A29sb6enpKCsrw7p16xAcHAwnJ6eGjE8pJycHS5cuRfPmzVXKY2Nj0bFjR6xatQoA4Ovri/Lycnz88ccIDw+HqakpEhISIJFIsH79epiamsLPzw/m5uZYsmQJJk2aBHt7e13sEpHeq+9C0gAXkyYi0kdPXOG1V69e6NWrFwBgz549GDhwIAIDAxsirireffdd9OjRA2ZmZjh79iwAoLS0FGfOnMHMmTNV6oaEhGDjxo04d+4cunbtiuPHj8Pf31/lcXWhoaFYvHgxUlNTMXjw4IbcFaJnRn0Xkga4mDQRkT5S+6/yDz/8AABQKBS4ePEibt26BVNTUzRv3hyurq5aCxAAEhMT8ccff+Cbb75BVFSUsjw7OxtyuRwODg4q9du2bQsAyMjIgLu7O+7cuVOljo2NDaysrJCRkaHV2ImIiIj0Ta2+lqekpGDx4sXIycmBIAgAHl0b+MILL+C9995DQECAxgO8desWli9fjuXLl1d5DF1RUREAwMrKSqXc0tISACCTyWqsU1lPJpPVKh5b26r9aJKdnQRCfgkkVuZ17sPExFin7fUhBu6D/sRgYWEGOxuLevUBPHpvkH7gWOgPjoVu1PQ5XZu/l5r621hXaieAZ86cwfTp02Fra4tZs2bB0dERgiDg+vXr2LlzJ2bMmIGtW7eic+fOGgtOEAT85z//gZ+fH0JCQqrd/iRGRkZq1amNvDwZKiqe3Gdd2dlJkJtbhJLSchTJHta5H7lct+31IYb6tpdYmes8hudhHACgpKQUuQpFvfqofG+Q7nEs9AfHQneq+5yWWJnX6u+lJv42PomRkeiJk1ZqJ4BxcXFo2bIlkpKSIJGofuMYMWIEBg8ejPj4eHzyySd1j/YxO3bsQHp6Ovbt24fy8kcXolcmdOXl5co4iouLVdpVzupJJBLlzN/jdSrrPb4vRERERM87tRPA8+fPY+rUqdUmTFZWVhgyZIhGkz8AOHjwIP766y/4+PhU2ebi4oL3338fYrEYWVlZKtsqXzs4OMDS0hL29vbIzMxUqZOXl4fi4uIq1wYSkWbV905iMxPeQEJEpGka+8sqEokgl8s11R0AYPHixVVm7tatW4e0tDSsXbsWrVq1wrfffotDhw5hzJgxEIlEAB4ljhKJRHlzSo8ePZCSkoK5c+cq7wQ+ePAgxGIxunTpotGYiUhVfe8k9u7AZZqIiDRN7QTQ3d0dSUlJGDFiBCwsVC9alMlkSExMRKdOnTQaXPv27auUNW3aFKampsrfNWXKFERERGDWrFl47bXX8Ouvv2LTpk14++230ahRIwDAhAkTsH//fkycOBFjxozBjRs3EB0djWHDhmn9ySVERERE+kbtBHDatGkYPXo0+vbti1GjRqFdu3YAoLwJJCcnB4sXL9ZWnDXq1q0b4uLiEBsbi6lTp8Le3h5z585VeRSco6MjNm/ejKioKMyYMQPW1taIiIjA9OnTGzxeIiIiIl1TOwH08vJCXFwcPvjgA0RFRSlPtwqCADs7O0RHR6Nr165aC7TSihUrqpQFBwcjODj4ie28vLzwxRdfaCssIiIiomdGra4BDAwMRK9evfDHH3/g5s2bAICWLVvCxcUFxsa8UJuIiIjoWVDrrE0sFsPNzQ1ubm7aiIeIiIiItIyPaCciIiIyMEwAiYiIiAwME0AiIiIiA8MEkIiIiMjAqJ0Ajh49Gr/88ovytUwmw+jRo3Hp0iWtBEZERERE2lHjXcA+Pj5wdXVFx44d0bFjR5w6dQrDhg1TbpfL5Th16hQKCwsbJFAiIiIi0owaE8AJEybg8uXLOHLkCBISEiASifDBBx/giy++QIcOHdC6dWuIRCLlgtBERERE9GyoMQEcO3as8v/Lysrg5uaGXr16wdLSEufPn0dSUhIEQcCUKVPg7OwMV1dXdOrUCf3792+IuImIiIiojtRaCNrU1BQA0LNnT/Tr1w8AkJ+fj+7du2PkyJFQKBT4448/kJyczASQiIiISM/VmAAOHToUHTp0gIuLCzp06AAAKqd7K/+/R48e6Natm5bDJCIiIiJNqTEBfOWVV5CWloYjR44gPz8fIpEIa9aswbFjx+Ds7IwWLVrwGkAiIiKiZ1CNCeCcOXOU/5+TkwM/Pz+89NJLePjwIT7//HPcvHkTADBv3jy4u7vD1dUVrq6u6N69u/ajJiIiIqI6U+saQHt7ewDAq6++qrwG8Pbt2wgICICvry8ePHiAL7/8EmvWrOG6gERERER6Tq0EEABatGgBCwsL5WsrKyu0aNECgwYNgqenJ4BHi0MTERERkX5TOwH84YcfVF43bty4SpmVlZVmoiIiIiIireGzgImIiIgMjNozgEREuiAyEuFefglKSsvr1N7MxBjG/KpLRKSCCSAR6bVSuQJpmfdQJHtYp/beHexhbMY/dURE/8a/ikT0XBMZiVBcx9nDSpxFJKLnDRNAInqulcoV+P1Kbr364CwiET1v+J2WiIiIyMAwASQiIiIyMEwAiYiIiAwME0AiIiIiA8MEkIiIiMjAMAEkIiIiMjBMAImIiIgMDBNAIiIiIgPDBJCIiIjIwDABJCIiIjIwTACJiIiIDAwTQCIiIiIDwwSQiIiIyMAY6zqAp6moqMDu3buxc+dO3Lx5E7a2tggMDMT06dNhZWUFALhw4QKioqJw8eJFWFpaYtCgQZg+fTpMTEyU/dy4cQMrVqzAmTNnIBaLERoainfeeUfZBxGRtpRXAKXy8jq3NzMxhjG/rhORBul9Arhx40asWbMG48ePR7du3ZCRkYHY2Fj8+eef2LRpEzIzMzF27Fh4enpizZo1uHbtGmJiYiCTybBo0SIAQGFhIcaMGQM7OzusXLkSeXl5WLVqFe7evYsNGzboeA+J6HlXKi/H6bScOrf37mAPYzO9/3NNRM8Qvf6LIggCNm7ciNdffx1vv/02AKB79+6wtrbGrFmzkJaWhu3bt0MikWD9+vUwNTWFn58fzM3NsWTJEkyaNAn29vbYsWMH/v77byQnJ8Pa2hoAYG9vj4kTJ+L333+Hu7u7LneTiIiIqEHp9UmF4uJi9O/fH3379lUpb9++PQAgKysLx48fh7+/P0xNTZXbQ0NDoVAokJqaCgA4fvw4vL29lckfAPj4+MDS0hLHjh1rgD0hIiIi0h96PQNoZWWFd999t0r5kSNHAACOjo64c+cOHBwcVLbb2NjAysoKGRkZAIDr16+jf//+KnXEYjFatWqlrENEVBORkQjFpXW/hq9C0GAwREQaoNcJYHV+//13JCQkICgoCI0bNwaAam/ksLS0hEwmAwAUFRU9tQ4RUU1K5Qr8fiW3zu3dpXYajIaIqP6eqQTw7NmzmDx5Mlq1aoUlS5agrKzsifWNjJ5+hludOv9ma6vdu4bt7CQQ8ksgsTKvcx8mJsY6ba8PMXAfnq8YANS5D33Zh/q0t7Awg52NRZ3ba5qdnUTXIdD/4VjoRk2f07V5n+v6ff3MJIAHDhzA/Pnz0a5dO2zcuBHW1tYoLi4GAOV//00mk0EiefTGsLKyqrFOixYtahVHXp4MFVo6n2NnJ0FubhFKSstRJHtY537kct2214cY6tteYmWu8xieh3HQVAwA6tyHvuxDfdqXlJQiV6Goc3tNqvw7RbrHsdCd6j6nJVbmtXqfa/t9bWQkeuKklV7fBFJpy5YtmD17Njw8PLBjxw688MILAB6dwrW3t0dmZqZK/by8PBQXFyuvDXRwcKhSR6FQ4ObNm1WuHyQiIiJ63ul9ApiYmIgVK1YgLCwMGzduVM7qVerRowdSUlJUTgcfPHgQYrEYXbp0UdY5efIkCgoKlHVSU1NRUlKC7t27N8h+EBEREekLvT4FnJeXh6VLl6Jly5YYOXIkLl26pLK9TZs2mDBhAvbv34+JEydizJgxuHHjBqKjozFs2DDl6d0RI0Zg+/btGDt2LKZOnYqCggKsWrUKvr6+6Ny5sy52jYhIbfW9Cxng00SISJVeJ4A//fQTHjx4gFu3bmHkyJFVtkdFRWHAgAHYvHkzoqKiMGPGDFhbWyMiIgLTp09X1rOxscHWrVuxbNkyzJkzB5aWlggNDcXcuXMbcneIiOqkvnchA0AXl+Yoldf9+mUmkETPF71OAAcOHIiBAwc+tZ6Xlxe++OKLJ9aRSqX49NNPNRMYEdEzpr5JJB9HR/R84fc5IiIiIgPDBJCIiIjIwDABJCIiIjIwTACJiIiIDAyv6CUioqeqXIpGyC9BSR2WpOFdxET6hQkgERE9VeVdxLV93FWl+i5DAzCJJNIkJoBERKR1mljLkEvREGkOv0sRERERGRgmgEREREQGhgkgERERkYHhxRRERPRMqLwTua54EwnRP5gAEhHRM4HPMybSHH4XIiIiIjIwTACJiIiIDAznwomIiNRUXgGUynkdIj37mAASEZFBqO9NJABQIQBnL+fUuT2vQyR9wX+FRERkEDTxNBJ3qZ2GoiHSLU5EExERERkYJoBEREREBoYJIBEREZGBYQJIREREZGCYABIREREZGN4FTERE1EDquxSNibEx5OWP2gv5JSipQ19ci5AAJoBEREQNpr5L0bhL7ZTtJVbmKJI9rHUfXVyao1Qu1DkGfUgguSB3/TEBJCIiMiD1TUL1IYEslZfjdBoX5K4Pw957IiIiqpX6JpBMvvQDR4CIiIgajKYeyUf1wwSQiIiIGgwfyacfDPwSSCIiIiLDwxlAIiIiMij1PQ39PJyCZgJIREREBkUTy/E863gKmIiIiMjAMAEkIiIiMjBMAImIiIgMDBNAIiIiIgPDBJCIiIjIwBhUAvjNN9+gT58+cHNzQ1hYGJKTk3UdEhEREVGDM5gE8MCBA5gzZw58fHywbt06dOnSBfPmzcN3332n69CIiIiIGpTBrAMYExODsLAwLFiwAADQs2dPFBYW4qOPPkJoaKiOoyMiIiJqOAYxA5idnY2srCz07t1bpTwkJATXr19Hdna2jiIjIiIiangGMQN4/fp1AICDg4NKedu2bQEAGRkZaN26tVp9GRmJNBtcNf0bi41gYW5S5z503V4fYqhv+0ZmxjqP4XkYB03F0MjMGIryuvWhL/vwPIyDhblJncdCn/bhWY7h3+11NRa6bq8PMVTXvrbjYSw20mpO8bS+RYIgPAdPtHuyb775Bm+//Ta+//57tGrVSlmemZmJ3r17IyYmBq+++qoOIyQiIiJqOAZxCvhpOa6RkUEcBiIiIiIABpIASiQSAEBxcbFKuUwmU9lOREREZAgMIgGsvPYvKytLpTwzM1NlOxEREZEhMIgEsG3btmjVqlWVNf8OHTqEdu3aoUWLFjqKjIiIiKjhGcRdwAAwdepULFiwAE2aNEGvXr3w/fff49tvv0VMTIyuQyMiIiJqUAZxF3Clzz//HJs3b8adO3fQunVrTJw4EQMHDtR1WEREREQNyqASQCIiIiIykGsAiYiIiOgfTACJiIiIDAwTQD3wzTffoE+fPnBzc0NYWBiSk5N1HdJzLS0tDS4uLrh7965KeWpqKgYPHgx3d3cEBARg8+bNVdpeuHAB4eHh8PT0hI+PD6KjoyGXyxsq9OdCRUUFdu3ahX79+sHT0xNBQUFYvny5cl1OQL3jfOPGDUyePBleXl545ZVX8N5776n0QeoRBAGffvopQkJC4Obmhv79+2Pfvn0qdfje0I1p06YhODhYpYxj0XDKy8vh5uYGJycnlR9PT09lnWd5PAzmLmB9deDAAcyZMwdjxoyBj48Pjhw5gnnz5sHc3ByhoaG6Du+5c+3aNUyaNAnl5eUq5efOncPkyZMRFhaGyMhInD17FlFRURAEAePHjwfwaN3IsWPHwtPTE2vWrMG1a9cQExMDmUyGRYsW6WJ3nkkbN27EmjVrMH78eHTr1g0ZGRmIjY3Fn3/+iU2bNql1nAsLCzFmzBjY2dlh5cqVyMvLw6pVq3D37l1s2LBBx3v4bNmwYQNiY2Mxffp0eHh44Mcff8ScOXMgFovx6quv8r2hI19//TUOHz6MNm3aKMs4Fg0rIyMDpaWlWLlyJdq1a6csr3x62DM/HgLpVFBQkDBz5kyVssjISCE0NFRHET2f5HK5sH37dsHT01Po0qWLIJVKhTt37ii3jxkzRhg6dKhKm6ioKMHLy0soLS0VBEEQ/vOf/wh+fn7K14IgCDt27BA6dOgg3L17t2F25BlXUVEheHt7C++//75K+f79+wWpVCpcunRJreO8bt06wcPDQ8jPz1fWOXr0qCCVSoXffvutYXbmOVBWViZ4e3sLH3zwgUr5qFGjhDfeeEMQBL43dOHu3buCt7e34OvrKwQFBSnLORYNa+/evYKzs7NQUlJS7fZnfTx4CliHsrOzkZWVhd69e6uUh4SE4Pr168jOztZRZM+fs2fPYvXq1Rg3bhzmzJmjsq20tBRnzpypdhz+/vtvnDt3DgBw/Phx+Pv7w9TUVFknNDQUCoUCqamp2t+J50BxcTH69++Pvn37qpS3b98ewKOn9ahznI8fPw5vb29YW1sr6/j4+MDS0hLHjh1rgD15PojFYmzbtg0TJ05UKTcxMUFpaSnfGzry7rvvokePHujWrZuyjGPR8NLS0tCmTRs0atSoyrbnYTyYAOrQ9evXAVR9FF3btm0BPJp+Js1wdHTEkSNHMG3aNIjFYpVt2dnZkMvlTxyHBw8e4M6dO1Xq2NjYwMrKimOlJisrK7z77rt4+eWXVcqPHDkC4NE4qXOcr1+/XqWOWCxGq1atOBa1YGRkBCcnJ9jb20MQBNy/fx8JCQn4+eef8frrr/O9oQOJiYn4448/8N///lelnGPR8NLT02Fqaorx48fD09MT3t7eWLRoEWQy2XMxHrwGUIeKiooAPPpQ/DdLS0sA4AXtGtSsWbMat6kzDjXVqazHsaq733//HQkJCQgKCkLjxo0BPP04FxUVcSw07NChQ5gxYwYAoFevXujfvz/S0tIA8L3RUG7duoXly5dj+fLlsLGxUdnGv1MN7/Lly5DJZBg6dCgmT56MixcvIi4uDhkZGZg9ezaAZ3s8mADqkPCUNbgrLzQl7VJnHDhW2nH27FlMnjwZrVq1wpIlS1BWVvbE+uocZ45F3XTs2BHbt29Heno6PvroI0ycOBGRkZFPbMP3huYIgoD//Oc/8PPzQ0hISLXbn4RjoXkxMTFo0qQJnJycAADe3t6wtbXFO++8g+PHjz+x7bMwHkwAdUgikQB4dF3Uv1V+K6jcTtqlzjhUfoN7vE5lPY5V7R04cADz589Hu3btsHHjRlhbWyuP79OOs5WVVY11WrRood3An1OtW7dG69at4e3tDSsrK8ybN0+5je8N7duxYwfS09Oxb98+5SoFlQlEeXk5/07pQJcuXaqU9erVS+X1szwe/DqgQ5XXBWRlZamUZ2Zmqmwn7WrTpg3EYnGVcah87eDgAEtLS9jb2yvHplJeXh6Ki4s5VrW0ZcsWzJ49Gx4eHtixYwdeeOEFAFD7ODs4OFSpo1AocPPmTY5FLRQUFCA5ORk5OTkq5R07dgQA3Lx5k++NBnLw4EH89ddf8PHxgYuLC1xcXJCcnIysrCy4uLjgzJkzHIsGlJeXh8TExCo3Yz58+BAAYGtr+8yPBxNAHWrbti1atWqF7777TqX80KFDaNeuHWcyGoiZmRm8vLxw6NAhlSn7gwcPQiKRwNXVFQDQo0cPpKSkqJymPHjwIMRicbXfFKl6iYmJWLFiBcLCwrBx48Yq34LVOc49evTAyZMnUVBQoKyTmpqKkpISdO/evUH243lQUVGB+fPnY/fu3Srllae3OnXqxPdGA1m8eDGSkpJUfvz9/dG8eXMkJSUhNDSUY9GARCIRFi1ahO3bt6uUHzhwAGKxGN27d3/2x0MXa8/QP7788ktBKpUKixcvFo4dOyYsWrRIkEqlwv79+3Ud2nOr8pj/ex3An3/+WXBychIiIyOFo0ePCjExMYKTk5OQkJCgrPPnn38KnTp1EsaMGSP88MMPwubNmwVXV1fhvffe08FePJvu378vuLu7C/7+/sLp06eFX3/9VeUnLy9PreOcl5cnvPLKK8KAAQOEQ4cOCV988YXg7e0tTJgwQXc794xavHix4OLiImzYsEH4+eefhbi4OMHV1VVYuHChIAh8b+jSvHnzVNYB5Fg0rP/9739Chw4dhNjYWOV7w8XFRViyZIkgCM/+eDAB1AO7du0SgoODBVdXVyEsLEzYs2ePrkN6rlWXAAqCIBw6dEjo27ev4OLiIgQEBAibNm2q0vb06dPC0KFDBVdXV6Fnz57Chx9+KJSVlTVU6M+8PXv2CFKptMaf5ORkQRDUO87p6enCmDFjBDc3N6Fbt27Cf//7X6GoqEgXu/VMKysrExISEoTevXsLrq6uQlBQkJCQkCAoFAplHb43dOPxBFAQOBYNqfK9ERISIri6ugqBgYHChg0bnpv3hkgQnnKbChERERE9V3gNIBEREZGBYQJIREREZGCYABIREREZGCaARERERAaGCSARERGRgWECSERERGRgmAASERERGRgmgESkMzKZDJs3b8agQYPw8ssvw8PDA0OGDMHu3btRUVGh6/B07vHnkFYnPDwcTk5ODRBN3clkMuTn5ytfz58/X+9jJnreMQEkIp24fv06Bg8ejOjoaDg5OWH27NmIjIyEmZkZFi1ahLlz58KQ16lfv349xo0bp+sw6u3ixYsICwvD1atXdR0KEf2Lsa4DICLDU1pairfeegsFBQVISkqCs7OzcltERAQWL16MnTt3ws3NDaNHj9ZhpLrzyy+/QKFQ6DqMerty5Qru3bun6zCI6DGcASSiBrdz505kZGRgwYIFKslfpXnz5qFJkyb4/PPPdRAdEdHzjwkgETW4/fv3w8LCAn369Kl2u7m5Ob744gskJyerlJ85cwZjx46Fp6cnPD09MXr0aJw+fVqlTkBAAD744AMkJiYiJCQEbm5uGDx4MM6fP4/c3FxERkbC09MTPXv2RHR0tMq1hk5OTvjkk0+QkJCAXr16wd3dHeHh4cjMzERGRgbGjx8PDw8PBAQEYOvWrVXi/uqrrzBw4EB06tQJXbt2xfz581Vmv27evAknJyckJycjJiYGvr6+6NSpE4YOHYoTJ06o7MOpU6dw69YtODk5IS4uri6HuYpff/0VERERyuM3btw4nD9/vsrxW7RoEb7++mv06dMHnTp1Qu/evbFjx44q/R07dgxDhw6Fh4cHAgMDsWPHDixcuBABAQEAgLi4OCxYsAAAMHr0aGV5pQsXLiA8PBxubm7o0aMHli1bhtLSUo3sKxE9mUgw5ItsiKjBCYIAV1dXdO7cGdu2bVO73ffff49p06ahTZs2GDx4MAAgMTERt2/fRmxsLAIDAwE8SmDKy8tRUVGBMWPGQBAExMfHo3HjxpBIJHjppZfQtWtXHDp0CKmpqVixYgVee+01AI8SQHt7ezRq1AgjR45EXl4eNm7cCEdHRxQUFKBXr17o2LEjEhMTcfHiRWzbtg1dunQBAKxduxZxcXEICQlB165dkZOTg+3bt6NJkyZISkqCjY0Nbt68icDAQLRo0QKNGjXCsGHDIJfLsXnzZpSUlODo0aOwtrbGkSNH8OGHH+Kvv/7CggUL4OTkVO1MKfDoJpBTp04hPT39icfv+PHjmDRpEpydndG3b1+UlZXhq6++wq1bt7BlyxZ4eXkpj58gCCguLsaoUaPQrFkz7N69G5cvX0ZCQgL8/PwAACkpKZg6dSqkUilee+015OTkYNu2bbCwsIClpSV++OEHXL58GTt37sTu3bsxefJkdOrUCUFBQZg/fz727NkDS0tL9O/fH87Ozjh69ChSUlIwevRoLFy4UO1/F0RURwIRUQPKy8sTpFKpMGvWLLXbyOVywdfXV/Dz8xOKioqU5YWFhULPnj2Fnj17CmVlZYIgCIK/v7/g5OQkXL58WVlv5cqVglQqFWbOnKksKy4uFlxcXITZs2cry6RSqeDu7i7k5uYqy2bMmCFIpVJh1apVyrIbN24IUqlUiI6OFgRBELKysgRnZ2dh9erVKnGnp6cLLi4uwtKlSwVBEITs7GxBKpUKfn5+QnFxsbLe/v37BalUKuzevVtZNmrUKMHf3/+px2bUqFGCVCp9Yh2FQiEEBgYKw4cPF8rLy1WOQXBwsDBgwABlWeXxS0tLU5bdu3dPcHJyUjlWQUFBQu/evYUHDx4oyw4fPixIpVKVuL/88ktBKpUKJ06cUJbNmzdPkEqlwpYtW1RiDA4OFvz8/J66z0RUfzwFTEQNysjo0Z+d2tzgcOnSJdy9excjR46ElZWVsrxx48YYNWoUcnJycPHiRWV5mzZtVJYZcXBwAAAEBwcryywsLGBra4vc3FyV3+Xp6YlmzZopX7dr165K21atWgGA8vTu4cOHUVFRgYCAAOTn5yt/mjVrhg4dOuDo0aMqv8PPzw8WFhbK15Wze4/HoimXLl1CdnY2goKCUFhYqIzv4cOH8Pf3R1paGnJycpT1HRwcVGYc7ezs0KxZM9y/fx8AcPnyZWRlZWH48OEwNzdX1gsKCkL79u3VjuvflwAYGRmhY8eOyt9BRNrFu4CJqEE1adIEJiYmKuvCPc3NmzcB/JPI/VtlwnH79m14enoCAGxtbVXqiMViAICNjU2VcuGxq2Aeb2tsbFylbWV/lW2zsrIAAMOHD682fhMTE5XXj8dhamoKAFpb+7AyvqioKERFRVVb5/bt27C3t682vsoYK+PLzMwEALRt27ZKvfbt2yMtLU2tuB4/1ubm5pDL5Wq1JaL6YQJIRA1KJBLB09MTFy9eRHl5uTLBelxMTAyys7OxYMGCJ64HWLnt30lWTX2KRKKnxleXtpWJUXx8vMqMWE0qZ0EbSmV8kZGR8PDwqLbOv2funhZfeXk5gH8S138zMzNTO66GPg5E9A8mgETU4IKDg3Hq1Cns378fAwYMqLL94cOHSEpKgkKhQNOmTdGyZUsAjxaPflxGRgYAoHnz5toN+gkq43vxxRfRoUMHlW3Hjh1TOW2tC5XxWVhYoHv37irbzp8/j8LCQrUS10qtW7cGANy4cQM+Pj4q227cuFG/YImoQfDrFxE1uNdffx0tW7ZEVFQUrly5orJNoVDg/fffx/379/Hmm2/CxMQELi4usLOzw65duyCTyZR1ZTIZdu7cCTs7O7i6ujb0bij5+/sDADZs2KAyW5mWloYpU6bgs88+q3WfRkZGGjsl7OrqCjs7O2zbtg3FxcXKcplMhpkzZ2LBggXK09rq9vfiiy8iKSkJZWVlyvLffvsNly5dUqlbOcvHR/sR6RfOABJRgzMzM8PatWsxbtw4DBkyBP369UOnTp1QUFCA7777DmlpaQgNDUVERASAR6d33333XcyaNQuDBw/GkCFDAABJSUm4d+8eYmNjdXo6USqVIjw8HNu2bUNBQQGCgoJQUFCA7du3w9LSEpGRkbXu08bGBqdPn8bmzZvx8ssvw93d/Yn1Fy1aVG35iBEj4OzsrDx+gwYNwpAhQ2BmZqZcRmf16tU1nvqujpGREebPn4+ZM2di+PDhGDBgAPLz87F169Yqp4UrryfctWsX7t+/j379+qn9e4hIe5gAEpFOdOzYEV9//TU+/fRT/Pjjjzhw4AAEQYCTkxOWLVuGQYMGqVx3FxoaiiZNmmD9+vVYt24djI2N4e7ujqVLlyrXsNOlhQsXon379vj888+xcuVKSCQSeHl5ITIyEo6OjrXub8KECUhPT0d0dDQGDRr01ARw9+7d1Zb7+vrC2dlZefzi4+Oxfv16GBkZ4aWXXkJ8fLxyBrM2QkNDERMTg/j4eKxatQr29vZYsGABkpOTVW7w6datG8LCwpCSkoITJ06gd+/etf5dRKR5XAiaiIhqRaFQoLCwsNq7hfv164fGjRtX++QQItIfvAaQiIhqRaFQwNfXt8pp5/T0dFy9ehVubm46ioyI1MVTwEREVCumpqYIDQ1FUlISRCIRXF1dce/ePezatQvW1tbKazeJSH/xFDAREdXaw4cPsWnTJuzduxd37tyBRCJBt27dMHPmTOWTUohIfzEBJCIiIjIwvAaQiIiIyMAwASQiIiIyMEwAiYiIiAwME0AiIiIiA8MEkIiIiMjA/H+K7IIXXguDqgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (10,5)\n",
    "\n",
    "# Truncate any comment lengths greater than 512.\n",
    "trunc_lengths = [min(l, 512) for l in lengths]\n",
    "\n",
    "# Plot the distribution of comment lengths.\n",
    "sns.distplot(trunc_lengths, kde=False, rug=False)\n",
    "\n",
    "# Alternatively, you might try using a log scale on the x-axis, but this is \n",
    "# tricky. See here for one approach:\n",
    "# https://stackoverflow.com/questions/47850202/plotting-a-histogram-on-a-log-scale-with-matplotlib?rq=1\n",
    "#plt.xscale('log')\n",
    "\n",
    "plt.title('Comment Lengths')\n",
    "plt.xlabel('Comment Length')\n",
    "plt.ylabel('# of Comments')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lH0TfSYpjYlx"
   },
   "source": [
    "### 3.3. `max_len` & GPU Memory Concerns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvuHdXi1CsBB"
   },
   "source": [
    "Given the higher percentage of longer-than-512-tokens samples, I thought it might make sense to try a maximum length of 512 for this example. However, with `batch_size = 32` and `max_len = 512`, I ran into a GPU out-of-memory error on the first iteration of the training loop! \n",
    "\n",
    "After performing some analysis and a number of experiments (which you can see in [part 3](https://colab.research.google.com/drive/1cAcQbTwmDpBYjPYLGRQnWhtBdx2_Aj1O) of the \"Case Study\"), here are some of my conclusions:\n",
    "\n",
    "1. If the combination of your `batch_size` and `max_len` are too high, then you can easily run out of memory (even on a Tesla K80 with 12GB!) when fine-tuning BERT-base. For example, with `batch_size = 16`, a `max_len = ~400` is about the limit on a Tesla K80. \n",
    "2. Reducing `batch_size` reduces memory consumption, but at the cost of slower training, and finding the best choice of `batch_size` appears to be important for accuracy.\n",
    "3. Reducing `max_len` reduces memory consumption and speeds up training, but has a substantial impact on BERT's accuracy on this benchmark. \n",
    "\n",
    "Again, for more details and analysis, see [part 3](https://colab.research.google.com/drive/1cAcQbTwmDpBYjPYLGRQnWhtBdx2_Aj1O) of the case study.\n",
    "\n",
    "I landed on `batch_size = 16` and `max_len = 300` for this example. The Tesla K80 has enough memory for this configuration, and BERT achieves higher accuracy then a simpler baseline classifier (more on that in \"Part III - Performance & Analysis\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "sPN9UmdHCv1w"
   },
   "outputs": [],
   "source": [
    "max_len = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7oQ69-4wU7v-"
   },
   "source": [
    "### 3.4. Perform Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIWAoWL2RK1p"
   },
   "source": [
    "Now we're ready to perform the real tokenization.\n",
    "\n",
    "The `tokenizer.encode_plus` function combines multiple steps for us:\n",
    "\n",
    "1. Split the sentence into tokens.\n",
    "2. Add the special `[CLS]` and `[SEP]` tokens.\n",
    "3. Map the tokens to their IDs.\n",
    "4. Pad or truncate all sentences to the same length.\n",
    "5. Create the attention masks which explicitly differentiate real tokens from `[PAD]` tokens.\n",
    "\n",
    "The first four features are in `tokenizer.encode`, but I'm using `tokenizer.encode_plus` to get the fifth item (attention masks). Documentation is [here](https://huggingface.co/transformers/main_classes/tokenizer.html?highlight=encode_plus#transformers.PreTrainedTokenizer.encode_plus).\n",
    "\n",
    "I generally try to avoid defining functions in my *tutorial* code, but I feel it's warranted here because we'll need to run this step twice--once for the training set and again for the test set--and it's a lot of code / comments to have duplicated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "1ijEWKSYUpHh"
   },
   "outputs": [],
   "source": [
    "def my_tokenize(comments, labels, max_len):\n",
    "    '''\n",
    "    Tokenize a dataset of comments.\n",
    "\n",
    "    Parameters:\n",
    "      `comments` - List of comments, represented as strings.\n",
    "        `labels` - List of integer labels for the corresponding comments.\n",
    "       `max_len` - Truncate all of the comments down to this length.\n",
    "    \n",
    "    Returns:\n",
    "      `input_ids` - All of the comments represented as lists of token IDs,\n",
    "                    padded out to `max_len`, and cast as a PyTorch tensor.\n",
    "         `labels` - The labels for the corresponding comments, formatted as \n",
    "                    a PyTorch tensor.\n",
    "      `attention_masks` - PyTorch tensor with the same dimensions as\n",
    "                          `input_ids`. For each token, simply indicates whether\n",
    "                           it is padding or not.\n",
    "    '''\n",
    "    # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    print('Tokenizing {:,} comments...'.format(len(comments)))\n",
    "\n",
    "    # For every comment (\"sentence\")...\n",
    "    for sent in comments:\n",
    "\n",
    "        # Report progress.\n",
    "        if ((len(input_ids) % 500) == 0):\n",
    "            print('  Tokenized {:,} comments.'.format(len(input_ids)))\n",
    "\n",
    "        # `encode_plus` will:\n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        #   (5) Pad or truncate the sentence to `max_length`\n",
    "        #   (6) Create attention masks for [PAD] tokens.\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            sent,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = max_len,      # Pad & truncate all sentences.\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                    )\n",
    "        \n",
    "        # Add the encoded sentence to the list.    \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        \n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "    # Convert the labels to a tensor.\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    return (input_ids, labels, attention_masks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TT4m3oytdVMK"
   },
   "source": [
    "Now we'll use the above function to actually perform the tokenization of the training comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "qLSoKse4dIYE",
    "outputId": "18a335c1-803f-4cab-92d8-b53ec80fe158"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing 11,314 comments...\n",
      "  Tokenized 0 comments.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafael/Documents/project_master_thesis/chemprot_testing_ground/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2073: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Tokenized 500 comments.\n",
      "  Tokenized 1,000 comments.\n",
      "  Tokenized 1,500 comments.\n",
      "  Tokenized 2,000 comments.\n",
      "  Tokenized 2,500 comments.\n",
      "  Tokenized 3,000 comments.\n",
      "  Tokenized 3,500 comments.\n",
      "  Tokenized 4,000 comments.\n",
      "  Tokenized 4,500 comments.\n",
      "  Tokenized 5,000 comments.\n",
      "  Tokenized 5,500 comments.\n",
      "  Tokenized 6,000 comments.\n",
      "  Tokenized 6,500 comments.\n",
      "  Tokenized 7,000 comments.\n",
      "  Tokenized 7,500 comments.\n",
      "  Tokenized 8,000 comments.\n",
      "  Tokenized 8,500 comments.\n",
      "  Tokenized 9,000 comments.\n",
      "  Tokenized 9,500 comments.\n",
      "  Tokenized 10,000 comments.\n",
      "  Tokenized 10,500 comments.\n",
      "  Tokenized 11,000 comments.\n"
     ]
    }
   ],
   "source": [
    "# Tokenize our entire training set.\n",
    "(train_input_ids, \n",
    " train_labels, \n",
    " train_attention_masks) = my_tokenize(train.data, train.target, max_len = max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xw-UzXM7dcuP"
   },
   "source": [
    "Let's take a peak at one of the training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "id": "y1CRcdfqdfGi",
    "outputId": "f6d374bf-6a9a-4ca9-d4de-ea8009079351"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "Token IDs: tensor([  101,  1045,  2001,  6603,  2065,  3087,  2041,  2045,  2071,  4372,\n",
      "         7138,  2368,  2033,  2006,  2023,  2482,  1045,  2387,  1996,  2060,\n",
      "         2154,  1012,  2009,  2001,  1037,  1016,  1011,  2341,  2998,  2482,\n",
      "         1010,  2246,  2000,  2022,  2013,  1996,  2397, 20341,  1013,  2220,\n",
      "        17549,  1012,  2009,  2001,  2170,  1037,  5318,  4115,  1012,  1996,\n",
      "         4303,  2020,  2428,  2235,  1012,  1999,  2804,  1010,  1996,  2392,\n",
      "        21519,  2001,  3584,  2013,  1996,  2717,  1997,  1996,  2303,  1012,\n",
      "         2023,  2003,  2035,  1045,  2113,  1012,  2065,  3087,  2064,  2425,\n",
      "         4168,  1037,  2944,  2171,  1010,  3194, 28699,  2015,  1010,  2086,\n",
      "         1997,  2537,  1010,  2073,  2023,  2482,  2003,  2081,  1010,  2381,\n",
      "         1010,  2030,  3649, 18558,  2017,  2031,  2006,  2023, 24151,  2559,\n",
      "         2482,  1010,  3531,  1041,  1011,  5653,  1012,   102,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', train.data[0])\n",
    "print('Token IDs:', train_input_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7npHp8DaNq-"
   },
   "source": [
    "Let's also tokenize the test dataset while we're at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "id": "6b7BbQbgaRVB",
    "outputId": "49558a0b-620a-4ca5-e5a2-62939bfb8599"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing 7,532 comments...\n",
      "  Tokenized 0 comments.\n",
      "  Tokenized 500 comments.\n",
      "  Tokenized 1,000 comments.\n",
      "  Tokenized 1,500 comments.\n",
      "  Tokenized 2,000 comments.\n",
      "  Tokenized 2,500 comments.\n",
      "  Tokenized 3,000 comments.\n",
      "  Tokenized 3,500 comments.\n",
      "  Tokenized 4,000 comments.\n",
      "  Tokenized 4,500 comments.\n",
      "  Tokenized 5,000 comments.\n",
      "  Tokenized 5,500 comments.\n",
      "  Tokenized 6,000 comments.\n",
      "  Tokenized 6,500 comments.\n",
      "  Tokenized 7,000 comments.\n",
      "  Tokenized 7,500 comments.\n"
     ]
    }
   ],
   "source": [
    "# Tokenize our entire test set.\n",
    "(test_input_ids, \n",
    " test_labels, \n",
    " test_attention_masks) = my_tokenize(test.data, test.target, max_len = max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wl12-BD637Ty"
   },
   "source": [
    "# Part II - BERT Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bwa6Rts-02-"
   },
   "source": [
    "## S4. Train Our Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xYQ3iLO08SX"
   },
   "source": [
    "Now that our input data is properly formatted, it's time to fine tune the BERT model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKetqAKJiwpj"
   },
   "source": [
    "### `check_gpu_mem`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDZZgG6ei1wF"
   },
   "source": [
    "The following function will return a table showing the GPU's current memory usage versus its total memory. It uses the `nvidia-smi` command line tool to retrieve this.\n",
    "\n",
    "Note that the memory values are in \"MiB\". 1 MiB = 2^20 bytes = 1,048,576 bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "kRJBBgfZiwpl"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "def check_gpu_mem():\n",
    "    '''\n",
    "    Uses Nvidia's SMI tool to check the current GPU memory usage.\n",
    "    Reported values are in \"MiB\". 1 MiB = 2^20 bytes = 1,048,576 bytes.\n",
    "    '''\n",
    "    \n",
    "    # Run the command line tool and get the results.\n",
    "    buf = os.popen('nvidia-smi --query-gpu=memory.total,memory.used --format=csv')\n",
    "\n",
    "    # Use csv module to read and parse the result.\n",
    "    reader = csv.reader(buf, delimiter=',')\n",
    "\n",
    "    # Use a pandas table just for nice formatting.\n",
    "    df = pd.DataFrame(reader)\n",
    "\n",
    "    # Use the first row as the column headers.\n",
    "    new_header = df.iloc[0] #grab the first row for the header\n",
    "    df = df[1:] #take the data less the header row\n",
    "    df.columns = new_header #set the header row as the df header\n",
    "\n",
    "    # Display the formatted table.\n",
    "    #display(df)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6TKgyUzPIQc"
   },
   "source": [
    "### 4.1. Load Pre-Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXYitPoE-cjH"
   },
   "source": [
    "As usual, for text classification, we'll be using [BertForSequenceClassification](https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNW2tFMZHedj"
   },
   "source": [
    "\n",
    "**Output Classifier**\n",
    "\n",
    "Because this is a **multi-class** example, the `transformers` class will add a **Softmax Regression classifier** onto the end of the pre-trained BERT model. \n",
    "\n",
    "The Softmax Regression classifier is designed to accomodate the fact that *only one class* is the correct label. It outputs probabilities for each class, which all sum to 1.0.  \n",
    "\n",
    "[![SoftMax Regression](https://drive.google.com/uc?export=view&id=1TlWV75wB_HZxB-JHhN_mWdqlwZRqVffS)](https://drive.google.com/uc?export=view&id=1TlWV75wB_HZxB-JHhN_mWdqlwZRqVffS)\n",
    "\n",
    "The fact that this is a multi-class example is communicated through the `num_labels = 20` parameter in the next code cell. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xFPk4RDHcnH"
   },
   "source": [
    "\n",
    "**Model Variant**\n",
    "\n",
    "We'll use \"bert-base-uncased\" for this example, the version that has only lowercase letters (\"uncased\") and is the smaller version of the two (\"base\" vs \"large\").\n",
    "\n",
    "The documentation for `from_pretrained` can be found [here](https://huggingface.co/transformers/v2.2.0/main_classes/model.html#transformers.PreTrainedModel.from_pretrained), with the additional parameters defined [here](https://huggingface.co/transformers/v2.2.0/main_classes/configuration.html#transformers.PretrainedConfig).\n",
    "\n",
    "During fine-tuning, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 115,
     "referenced_widgets": [
      "e56d11573409479c96fa61299e531ff2",
      "48efb8fcc58948a19d7a42f5ddf439b0",
      "bc44a9a24edd4a2b81f0abb6ea8e0a1a",
      "904869e0fdf74b699d91685dd77cf488",
      "93180cc77d384fb09b400b3acd6dfade",
      "293440e19666450d9e93cadb10af6dc8",
      "5af3ce0b7305447fba23a6ef42ea8d8a",
      "955be4821556481c9bf509520e704339",
      "b4a3e68c591a4684a05674383833c327",
      "8615f37fe15749bbb229bbfa71d55bd8",
      "eafc86cf216a4ed48dc2b42807b631b6",
      "e4435d1129bd4db1a0731f13f51b2cb9",
      "9707e35d55c14608be12a2115cb2cb3f",
      "cb2ed22245e04baa88c20a5fc82609db",
      "770a584c4966492a917e8b5aa4127b70",
      "aa335525f2ee43e198e09a3820dfdfdd"
     ]
    },
    "id": "gFsCTp_mporB",
    "outputId": "1aba405f-90a4-4e0b-aee7-59452bb8260b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 22:22:18.205336: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 20, # For our 20 newsgroups!\n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "desc = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "id": "lbmX98ovPy2d",
    "outputId": "5bda0b40-8fb5-4452-9001-7bb43069c937"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>memory.total [MiB]</th>\n",
       "      <th>memory.used [MiB]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5917 MiB</td>\n",
       "      <td>2403 MiB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0 memory.total [MiB]  memory.used [MiB]\n",
       "1           5917 MiB           2403 MiB"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_gpu_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aRp4O7D295d_"
   },
   "source": [
    "### 4.2. Splitting off a Validation Set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qu0ao7p8rb06"
   },
   "source": [
    "This dataset already has a train / test split, but we're going to further divide up our training set to use 90% for training and 10% for *validation*. This validation set will help us detect over-fitting during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "GEgLpFVlo1Z-",
    "outputId": "5fadb120-dead-4c55-928f-44c8b4399847"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10,182 training samples\n",
      "1,132 validation samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "\n",
    "# Create a 90-10 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejHQThm_uVnB"
   },
   "source": [
    "### 4.3. Batch Size & DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dD9i6Z2pG-sN"
   },
   "source": [
    "We'll also create an iterator for our dataset using the torch DataLoader class. This helps save on memory during training because, unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory.\n",
    "\n",
    "This is also the point at which we must specify our **batch size**, since this is integral to the data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "XGUqOCtgqGhP"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 16\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRWT-D4U_Pvx"
   },
   "source": [
    "### 4.4. Optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8o-VEBobKwHk"
   },
   "source": [
    "Our `model` object handles the execution of a forward pass, and the calculation of gradients during training. \n",
    "\n",
    "The actual updates to the model's weights, however, are performed by an Optimizer object. Here, we create that object and give it a reference to our model's parameters, as well as set some of our training hyperparameters.\n",
    "\n",
    "For the purposes of fine-tuning, the authors recommend choosing from the following values:\n",
    "- Batch size: 16, 32 \n",
    "- Learning rate (Adam): 5e-5, 3e-5, 2e-5  (We'll use 2e-5).\n",
    "- Number of epochs: 2, 3, 4  (We'll use 4). This is a parameter to the \"learning rate scheduler\" in the section.\n",
    "\n",
    "The epsilon parameter `eps = 1e-8` is \"a very small number to prevent any division by zero in the implementation\" (from [here](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)).\n",
    "\n",
    "You can find the creation of the AdamW optimizer in `run_glue.py` [here](https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L109)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "GLs72DuMODJO"
   },
   "outputs": [],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_iaG0A5quuqz"
   },
   "source": [
    "### 4.5. Epochs & Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGeSUdv_wDgF"
   },
   "source": [
    "The learning rate scheduler is responsible for updating the learning rate over the course of the training. Generally speaking, you want the learning rate to gradually get smaller and smaller so that training makes gradually finer adjustments to the weights. \n",
    "\n",
    "This decay needs to happen *across all of the training epochs*, so this is where we need to specify the number of epochs we want to train for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "-p0upAhhRiIx"
   },
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_QXZhFb4LnV5"
   },
   "source": [
    "**Helper Functions**\n",
    "\n",
    "In each pass, we will train the model on our full training set, and then measure it's accuracy on our 10% holdout validation set. We'll define a helper function for calculating accuracy on the *validation* set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "9cQNvaZ9bnyy"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNhRtWPXH9C3"
   },
   "source": [
    "Helper function for formatting elapsed times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "gpt6tR83keZD"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqfmWwUR_Sox"
   },
   "source": [
    "### 4.5. Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfNIhN19te3N"
   },
   "source": [
    "We're ready to kick off the training!\n",
    "\n",
    "For addressing our **GPU memory issue**, I've added some calls to `check_gpu_mem` during the training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WiDKq4cLQG6H",
    "outputId": "76e2089b-5792-48a4-d8b1-a0e5772197cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "\n",
      "  Step 0 GPU Memory Use:\n",
      "    Before forward-pass:  2405 MiB\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 5.78 GiB total capacity; 3.55 GiB already allocated; 32.12 MiB free; 3.75 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26319/2993038373.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# the loss (because we provided labels) and the \"logits\"--the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m# outputs prior to activation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         loss, logits = model(b_input_ids, \n\u001b[0m\u001b[1;32m     88\u001b[0m                              \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                              \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb_input_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/project_master_thesis/chemprot_testing_ground/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/project_master_thesis/chemprot_testing_ground/venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1499\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1501\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1502\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/project_master_thesis/chemprot_testing_ground/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/project_master_thesis/chemprot_testing_ground/venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    969\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m         )\n\u001b[0;32m--> 971\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    972\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/project_master_thesis/chemprot_testing_ground/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/project_master_thesis/chemprot_testing_ground/venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    566\u001b[0m                 )\n\u001b[1;32m    567\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    569\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/project_master_thesis/chemprot_testing_ground/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/project_master_thesis/chemprot_testing_ground/venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    457\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/project_master_thesis/chemprot_testing_ground/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/project_master_thesis/chemprot_testing_ground/venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     ):\n\u001b[0;32m--> 387\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    388\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/project_master_thesis/chemprot_testing_ground/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/project_master_thesis/chemprot_testing_ground/venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;31m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"relative_key\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"relative_key_query\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 5.78 GiB total capacity; 3.55 GiB already allocated; 32.12 MiB free; 3.75 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Check GPU memory for the first couple steps.\n",
    "        if step < 2:\n",
    "            print('\\n  Step {:} GPU Memory Use:'.format(step))\n",
    "            df = check_gpu_mem()    \n",
    "            print('    Before forward-pass: {:}'.format(df.iloc[0, 1]))\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        # It returns different numbers of parameters depending on what arguments\n",
    "        # arge given and what flags are set. For our useage here, it returns\n",
    "        # the loss (because we provided labels) and the \"logits\"--the model\n",
    "        # outputs prior to activation.\n",
    "        loss, logits = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "\n",
    "        # Report GPU memory use for the first couple steps.\n",
    "        if step < 2:\n",
    "            df = check_gpu_mem()    \n",
    "            print('     After forward-pass: {:}'.format(df.iloc[0, 1]))\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Report GPU memory use for the first couple steps.\n",
    "        if step < 2:\n",
    "            df = check_gpu_mem()    \n",
    "            print('    After gradient calculation: {:}'.format(df.iloc[0, 1]))\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "            # values prior to applying an activation function like the softmax.\n",
    "            (loss, logits) = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQTvJ1vRP7u4"
   },
   "source": [
    "**Check for Over-Fitting**\n",
    "\n",
    "Let's view the summary of the training process.\n",
    "\n",
    "Training accuracy should always go up, or at least remain constant, with each additional training epoch. \n",
    "\n",
    "However, if we see that the validation accuracy is *decreasing*, it tells us that we are training too long, and our model is over-fitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "6O_NbXFGMukX",
    "outputId": "e1a7e432-d549-4573-d179-66d7ba08cba5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Valid. Loss</th>\n",
       "      <th>Valid. Accur.</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Validation Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.61</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0:09:42</td>\n",
       "      <td>0:00:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0:09:48</td>\n",
       "      <td>0:00:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0:09:48</td>\n",
       "      <td>0:00:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0:09:48</td>\n",
       "      <td>0:00:24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
       "epoch                                                                         \n",
       "1               1.61         1.04           0.70       0:09:42         0:00:24\n",
       "2               0.82         0.93           0.72       0:09:48         0:00:24\n",
       "3               0.57         0.89           0.74       0:09:48         0:00:24\n",
       "4               0.43         0.90           0.74       0:09:48         0:00:24"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame from our training statistics.\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "# A hack to force the column headers to wrap.\n",
    "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '50px')])])\n",
    "\n",
    "# Display floats with two decimal places.\n",
    "pd.set_option('precision', 2)\n",
    "\n",
    "# Display the table.\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wC64axhF0_aQ"
   },
   "source": [
    "Let's take a look at our training loss over all epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "id": "kOx9lsfm0_aR",
    "outputId": "f5d5b29f-da5f-4716-f0b5-eb19e3ab335a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training & Validation loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([1, 2, 3, 4])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LqONpNsvA29f"
   },
   "source": [
    "We do appear to be over-fitting slightly in our fourth epoch, but not severely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HDNPfN7qSfAY"
   },
   "source": [
    "# Part III - Performance & Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mkyubuJSOzg3"
   },
   "source": [
    "## S5. Performance On Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "huAaDukC1Orh"
   },
   "source": [
    "### 5.1. Baseline Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TELGvAm56e0T"
   },
   "source": [
    "The below code is taken directly from the sci-kit learn example code for this dataset [here](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html). I've re-organized it a bit and added comments.  \n",
    "\n",
    "The quick summary:\n",
    "* It uses `tf-idf` for vectorization.\n",
    "* It uses `multinomial naive bayes` for classification.\n",
    "* F1 score: `0.683`   (on the test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "id": "bcGu2FTQ1S7w",
    "outputId": "ae2aae53-12a6-45b8-c3f4-f7caa2f8aa12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing dataset for tf-idf...\n",
      "\n",
      "Vectorizing the text samples...\n",
      "\n",
      "11,314 train samples with 101,631 features.\n",
      " 7,532  test samples with 101,631 features.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "# Rename our dataset variables to be consistent with the example.\n",
    "newsgroups_train = train\n",
    "newsgroups_test = test\n",
    "\n",
    "# Vectorize the text using tf-idf.\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Before we can vectorize the text, the tf-idf model needs\n",
    "# to analyze the dataset to build a vocabulary and word counts.\n",
    "print('Analyzing dataset for tf-idf...\\n')\n",
    "vectorizer.fit(newsgroups_train.data)\n",
    "\n",
    "# Convert the text samples into tf-idf vectors.\n",
    "print('Vectorizing the text samples...\\n')\n",
    "vectors_train = vectorizer.transform(newsgroups_train.data)\n",
    "vectors_test = vectorizer.transform(newsgroups_test.data)\n",
    "\n",
    "print('{:>6,} train samples with {:<7,} features.'.format(vectors_train.shape[0], vectors_train.shape[1]))\n",
    "print('{:>6,}  test samples with {:<7,} features.'.format(vectors_test.shape[0], vectors_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "id": "Mvutv2D53bJo",
    "outputId": "f8d518d9-08dc-4bc9-d500-6dfe069ab05d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Naive Bays classifier...\n",
      "\n",
      "Predicting labels for the test set...\n",
      "\n",
      "F1 score: 0.683\n"
     ]
    }
   ],
   "source": [
    "# Following the example, we'll use a \"multinomial Naive Bayes\" classifier.\n",
    "clf = MultinomialNB(alpha=.01)\n",
    "\n",
    "# Train the classifier on the training set.\n",
    "print('Training the Naive Bays classifier...\\n')\n",
    "clf.fit(vectors_train, newsgroups_train.target)\n",
    "\n",
    "# Run prediction on the test set.\n",
    "print('Predicting labels for the test set...\\n')\n",
    "pred = clf.predict(vectors_test)\n",
    "\n",
    "# Use the F1 metric to score our classifier's performance on the test set.\n",
    "score = metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
    "\n",
    "# Print the F1 score!\n",
    "print('F1 score: {:.3}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16lctEOyNFik"
   },
   "source": [
    "### 5.2. Evaluate BERT on Test Set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhR99IISNMg9"
   },
   "source": [
    "\n",
    "Now we can apply our fine-tuned BERT model to generate predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "Hba10sXR7Xi6",
    "outputId": "b2a6dc50-f5dc-44e5-bb7d-a3cabe355f0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 7,532 test comments...\n",
      "  Batch    50  of    471.    Elapsed: 0:00:17.\n",
      "  Batch   100  of    471.    Elapsed: 0:00:34.\n",
      "  Batch   150  of    471.    Elapsed: 0:00:51.\n",
      "  Batch   200  of    471.    Elapsed: 0:01:07.\n",
      "  Batch   250  of    471.    Elapsed: 0:01:24.\n",
      "  Batch   300  of    471.    Elapsed: 0:01:41.\n",
      "  Batch   350  of    471.    Elapsed: 0:01:58.\n",
      "  Batch   400  of    471.    Elapsed: 0:02:14.\n",
      "  Batch   450  of    471.    Elapsed: 0:02:31.\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "# Create the DataLoaders.\n",
    "\n",
    "# Combine the features into a dataset object.\n",
    "test_data = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
    "\n",
    "# Create a sequential sampler--no need to randomize the order!\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "\n",
    "# Create the data loader.\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# Predict labels for all test set examples.\n",
    "\n",
    "print('Predicting labels for {:,} test comments...'.format(len(test_input_ids)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Measure elapsed time.\n",
    "t0 = time.time()\n",
    "\n",
    "# Predict \n",
    "for (step, batch) in enumerate(test_dataloader):\n",
    "    \n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "    # Progress update every 50 batches.\n",
    "    if step % 50 == 0 and not step == 0:\n",
    "        # Calculate elapsed time in minutes.\n",
    "        elapsed = format_time(time.time() - t0)\n",
    "        \n",
    "        # Report progress.\n",
    "        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\n",
    "\n",
    "\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "  \n",
    "    # Telling the model not to compute or store the compute graph, saving memory\n",
    "    # and speeding up prediction\n",
    "    with torch.no_grad():\n",
    "        # Forward pass, calculate logit predictions\n",
    "        outputs = model(b_input_ids, token_type_ids=None, \n",
    "                        attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JrBswmqmyH5m"
   },
   "source": [
    "To turn our model outputs into actual predictions, we need to:\n",
    "\n",
    "1. Re-combine the predictions from across all of the batches.\n",
    "2. Pick a class label for each prediction by choosing the class with the highest output value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "WlBi2hc-IuVu",
    "outputId": "a0da9bf0-53d6-41de-907f-9c240a5654aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`predictions` has shape (7532, 20)\n",
      "`predicted_labels` has shape (7532,)\n"
     ]
    }
   ],
   "source": [
    "# Combine the results across the batches.\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "# Take the highest scoring output as the predicted label.\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "print('`predictions` has shape', predictions.shape)\n",
    "print('`predicted_labels` has shape', predicted_labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMVyXXHlr2pt"
   },
   "source": [
    "Let's peek at the model's outputs for the first 10 test samples, along with the correct labels for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "_mjLBPw_LIE0",
    "outputId": "21e91363-89d7-4663-ee05-828672454613"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [ 7  5  0 17  0 13  0 15  5  1]\n",
      "  Correct: [ 7  5  0 17 19 13 15 15  5  1]\n"
     ]
    }
   ],
   "source": [
    "# Reduce printing precision for legibility.\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "print(\"Predicted:\", str(predicted_labels[0:10]))\n",
    "print(\"  Correct:\", str(true_labels[0:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3_xUMQEy2nC"
   },
   "source": [
    "The metric for the 20 Newsgroups dataset used by the scikit-learn example is the F1 score. Let's see how BERT performed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "TAm_DiwRD6e5",
    "outputId": "db65224b-7031-4811-ba60-a6e92ca7b6f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.693\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Use the F1 metric to score our classifier's performance on the test set.\n",
    "score = metrics.f1_score(true_labels, predicted_labels, average='macro')\n",
    "\n",
    "# Print the F1 score!\n",
    "print('F1 score: {:.3}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WrMJKgif08iu"
   },
   "source": [
    "At this point, we have the following F1 scores:\n",
    "\n",
    "* Naive Bayes: `0.683`\n",
    "* BERT:  `0.693`\n",
    "\n",
    "We've outperformed the baseline! \n",
    "\n",
    "Using `batch_size = 8` and `max_len = 512`, the accuracy further increased to an F1 score of `0.701`. However, the training time was around 2 hours on a Tesla K80, so I stuck with `batch_size = 16` and `max_len = 300` for this example."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT Multi-Class Classification - 20 Newsgroups.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "293440e19666450d9e93cadb10af6dc8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "48efb8fcc58948a19d7a42f5ddf439b0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4ddf121894674a5593b5acb284b9bca6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "587a51577f7148a4a2c685b6ccd863d0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5af3ce0b7305447fba23a6ef42ea8d8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "770a584c4966492a917e8b5aa4127b70": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8615f37fe15749bbb229bbfa71d55bd8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "891297991ff240b2ba80f3f171558e60": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "904869e0fdf74b699d91685dd77cf488": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_955be4821556481c9bf509520e704339",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_5af3ce0b7305447fba23a6ef42ea8d8a",
      "value": " 433/433 [00:08&lt;00:00, 48.8B/s]"
     }
    },
    "93180cc77d384fb09b400b3acd6dfade": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "955be4821556481c9bf509520e704339": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9707e35d55c14608be12a2115cb2cb3f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "9dc1c087b64e4351aa7c897bb1e505b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_587a51577f7148a4a2c685b6ccd863d0",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_d88d2ed6ca5a4db4975ddbab3c574111",
      "value": " 232k/232k [00:00&lt;00:00, 593kB/s]"
     }
    },
    "a389ea538d714857896b5c56d5a8eb57": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "aa335525f2ee43e198e09a3820dfdfdd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b4a3e68c591a4684a05674383833c327": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_eafc86cf216a4ed48dc2b42807b631b6",
       "IPY_MODEL_e4435d1129bd4db1a0731f13f51b2cb9"
      ],
      "layout": "IPY_MODEL_8615f37fe15749bbb229bbfa71d55bd8"
     }
    },
    "bc44a9a24edd4a2b81f0abb6ea8e0a1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_293440e19666450d9e93cadb10af6dc8",
      "max": 433,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_93180cc77d384fb09b400b3acd6dfade",
      "value": 433
     }
    },
    "c9b37fb4de0b41459e754ee3b12294ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_891297991ff240b2ba80f3f171558e60",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a389ea538d714857896b5c56d5a8eb57",
      "value": 231508
     }
    },
    "cb2ed22245e04baa88c20a5fc82609db": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d88d2ed6ca5a4db4975ddbab3c574111": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "db72db3535264504963af4b5cad88adf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c9b37fb4de0b41459e754ee3b12294ba",
       "IPY_MODEL_9dc1c087b64e4351aa7c897bb1e505b4"
      ],
      "layout": "IPY_MODEL_4ddf121894674a5593b5acb284b9bca6"
     }
    },
    "e4435d1129bd4db1a0731f13f51b2cb9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aa335525f2ee43e198e09a3820dfdfdd",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_770a584c4966492a917e8b5aa4127b70",
      "value": " 440M/440M [00:06&lt;00:00, 71.1MB/s]"
     }
    },
    "e56d11573409479c96fa61299e531ff2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bc44a9a24edd4a2b81f0abb6ea8e0a1a",
       "IPY_MODEL_904869e0fdf74b699d91685dd77cf488"
      ],
      "layout": "IPY_MODEL_48efb8fcc58948a19d7a42f5ddf439b0"
     }
    },
    "eafc86cf216a4ed48dc2b42807b631b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cb2ed22245e04baa88c20a5fc82609db",
      "max": 440473133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9707e35d55c14608be12a2115cb2cb3f",
      "value": 440473133
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
